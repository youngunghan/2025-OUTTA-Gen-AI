{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##CLIP 클래스 구현"
      ],
      "metadata": {
        "id": "GP2VLQhb8DU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amCfc6Uw6UX1"
      },
      "outputs": [],
      "source": [
        "#필요 패키지 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP 모델 구현\n",
        "class CLIP(nn.Module):\n",
        "    def __init__(self, embed_dim=512, image_resolution=224, vision_layers=12,\n",
        "                 vision_width=768, vision_patch_size=32, context_length=77,\n",
        "                 vocab_size=49408, transformer_width=512, transformer_heads=8,\n",
        "                 transformer_layers=12):\n",
        "        super().__init__()\n",
        "\n",
        "        # 이미지 인코더 (Vision Transformer 간소화 버전)\n",
        "        self.visual = VisionTransformer(\n",
        "            input_resolution=image_resolution,\n",
        "            patch_size=vision_patch_size,\n",
        "            width=vision_width,\n",
        "            layers=vision_layers,\n",
        "            output_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # 텍스트 인코더 (Transformer 간소화 버전)\n",
        "        self.transformer = Transformer(\n",
        "            width=transformer_width,\n",
        "            layers=transformer_layers,\n",
        "            heads=transformer_heads,\n",
        "            context_length=context_length\n",
        "        )\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.positional_embedding = nn.Parameter(torch.empty(context_length, transformer_width))\n",
        "        self.ln_final = nn.LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "        nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
        "\n",
        "    def encode_image(self, image):\n",
        "        return self.visual(image)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        x = self.token_embedding(text)\n",
        "        x = x + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        image_features = self.encode_image(image)\n",
        "        text_features = self.encode_text(text)\n",
        "\n",
        "        # 정규화\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # 코사인 유사도 계산을 위한 로짓\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text\n"
      ],
      "metadata": {
        "id": "J9-cM0j_7Oxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ViT 클래스\n",
        "#논문에서는 ResNet50 역시 사용했으나 결과적으로 성능이 좋았던 것은 ViT\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, input_resolution=224, patch_size=32, width=768, layers=12, output_dim=512):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # 간소화를 위해 ResNet을 backbone으로 사용\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone.fc = nn.Identity()  # FC 레이어 제거\n",
        "\n",
        "        self.proj = nn.Linear(2048, output_dim)  # ResNet50의 마지막 레이어 출력 차원은 2048\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ylb183qC7Rph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트 데이터를 위한 클래스\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, width=512, layers=12, heads=8, context_length=77):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.layers = layers\n",
        "\n",
        "        # 간소화를 위해 transformer 레이어 직접 구현 대신 nn.TransformerEncoder 사용\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=width, nhead=heads)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)"
      ],
      "metadata": {
        "id": "1iWKalL57TYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP 손실 함수\n",
        "def contrastive_loss(logits_per_image, logits_per_text, labels):\n",
        "    loss_image = F.cross_entropy(logits_per_image, labels)\n",
        "    loss_text = F.cross_entropy(logits_per_text, labels)\n",
        "    return (loss_image + loss_text) / 2.0"
      ],
      "metadata": {
        "id": "i7TjfrBC7fYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP을 위한 데이터셋 클래스\n",
        "# 데이터셋 구축 : 대규모의 이미지-텍스트 쌍을 생성해야함\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, processor, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.processor = processor\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                                (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        caption = self.captions[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # 텍스트 토큰화 (실제 구현에서는 tokenizer 사용)\n",
        "        text = torch.zeros(77, dtype=torch.long)  # 더미 토큰\n",
        "\n",
        "        return image, text"
      ],
      "metadata": {
        "id": "87de7S1f7hzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 예시\n",
        "def train_clip(model, dataloader, optimizer, device, epochs=1):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for images, texts in dataloader:\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "            # 대각 행렬의 인덱스가 올바른 쌍이라고 가정\n",
        "            labels = torch.arange(images.shape[0], device=device)\n",
        "\n",
        "            loss = contrastive_loss(logits_per_image, logits_per_text, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "c4-kj72m7oEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# 모델 사용 예시\n",
        "def main():\n",
        "    # 모델 생성\n",
        "    model = CLIP(embed_dim=512)\n",
        "\n",
        "    # 데이터셋 및 데이터 로더 설정 (더미 데이터)\n",
        "    image_paths = [\"image1.jpg\", \"image2.jpg\"]  # 예시 경로\n",
        "    captions = [\"a dog\", \"a cat\"]  # 예시 캡션\n",
        "\n",
        "    dataset = CLIPDataset(image_paths, captions, None)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # 옵티마이저 설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # 학습\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_clip(model, dataloader, optimizer, device, epochs=5)\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), \"clip_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''"
      ],
      "metadata": {
        "id": "71ES6LCa7smF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##데이터셋 로드"
      ],
      "metadata": {
        "id": "ZtRSRyvQ8GP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ngdQTe8M7V",
        "outputId": "e2bc6f64-8959-4217-c83e-ab1be3914537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# MS COCO 캡션 데이터셋 로드 (2017 버전, Train split)\n",
        "coco_dataset = load_dataset(\"imagecaptioning/mscoco_caption\", split=\"train\")\n",
        "\n",
        "# 첫 번째 예시 확인\n",
        "print(coco_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "yJuiyvXL8HqU",
        "outputId": "e4404a65-c06c-4158-dd3a-d14b98c859b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find a dataset script at /content/imagecaptioning/mscoco_caption/mscoco_caption.py or any data file in the same directory. Couldn't find 'imagecaptioning/mscoco_caption' on the Hugging Face Hub either: FileNotFoundError: Dataset 'imagecaptioning/mscoco_caption' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9699967cbc12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# MS COCO 캡션 데이터셋 로드 (2017 버전, Train split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcoco_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imagecaptioning/mscoco_caption\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 첫 번째 예시 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1492\u001b[0m                         \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /content/imagecaptioning/mscoco_caption/mscoco_caption.py or any data file in the same directory. Couldn't find 'imagecaptioning/mscoco_caption' on the Hugging Face Hub either: FileNotFoundError: Dataset 'imagecaptioning/mscoco_caption' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8Z6Iql_8REs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}