{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "19kydwCFkswd",
        "yjdEvXQakuSG",
        "nPG8SbL2k0VT",
        "U8ROB36-qBqI",
        "xZGJFTAz-Mtj",
        "h2-STEAVCpyJ",
        "rUWp0krKXYC4",
        "SacXPNb5AQdB"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Implementation\n",
        "\n",
        "This code implements **DPO** algorithm to finetune simple trained LM.\n",
        "\n",
        "We trained a small transformer model with \"TinyShakespeare\" dataset. Then we finetune the model with DPO algorithm so that the finetuned model barely generate the text **containing the same word more than twice**.\n",
        "\n",
        "So the total algorithm proceeds as follows.\n",
        "\n",
        "1. Prepare TinyShakespeare dataset and base model.\n",
        "2. Train the base model with the dataset.\n",
        "3. Define the reward function to score each text based on the number of repeated words in the text.\n",
        "4. Using the trained model(reference model), generate the texts and calculate the scores so that we can organnize preference dataset.\n",
        "5. With the organized preference dataset, finetune the reference model using the DPO algorithm."
      ],
      "metadata": {
        "id": "Skp0oNjKAfR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "19kydwCFkswd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3qVGRwWgEh5",
        "outputId": "b3897b5d-3628-426d-9148-d409fa5d2be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/논문코드리뷰/논문코드리뷰_DPO SimPO\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Literal, Callable\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/논문코드리뷰/논문코드리뷰_DPO SimPO\n",
        "# Get TinyShakeSpeare dataset\n",
        "train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model"
      ],
      "metadata": {
        "id": "yjdEvXQakuSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CharTokenizer:\n",
        "    # Character-level tokenizer\n",
        "    stoi: Dict[str, int]\n",
        "    itos: Dict[int, str]\n",
        "    pad_id: int\n",
        "    bos_id: int\n",
        "    eos_id: int\n",
        "\n",
        "    def encode(self, s: str, add_bos: bool = False, add_eos: bool = False) -> List[int]:\n",
        "        ids: List[int] = []\n",
        "        if add_bos:\n",
        "            ids.append(self.bos_id)\n",
        "        ids.extend(self.stoi[c] for c in s)\n",
        "        if add_eos:\n",
        "            ids.append(self.eos_id)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int], skip_special: bool = True) -> str:\n",
        "        out = []\n",
        "        for i in ids:\n",
        "            if skip_special and i in (self.pad_id, self.bos_id, self.eos_id):\n",
        "                continue\n",
        "            out.append(self.itos[int(i)])\n",
        "        return \"\".join(out)\n",
        "\n",
        "\n",
        "def build_char_tokenizer_from_text(text: str) -> CharTokenizer:\n",
        "    specials = [\"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
        "    vocab = specials + sorted(set(text))\n",
        "    stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "    itos = {i: ch for ch, i in stoi.items()}\n",
        "    return CharTokenizer(\n",
        "        stoi=stoi,\n",
        "        itos=itos,\n",
        "        pad_id=stoi[\"<PAD>\"],\n",
        "        bos_id=stoi[\"<BOS>\"],\n",
        "        eos_id=stoi[\"<EOS>\"],\n",
        "    )\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(C, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        scores = scores.masked_fill(attn_mask == 0, float(\"-inf\"))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.ln1(x), attn_mask)\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyCausalTransformerLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_seq_len: int = 256,\n",
        "        d_model: int = 256,\n",
        "        n_layers: int = 6,\n",
        "        n_heads: int = 8,\n",
        "        d_ff: int = 1024,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m: nn.Module):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _causal_mask(self, T: int, device: torch.device) -> torch.Tensor:\n",
        "        mask = torch.tril(torch.ones((T, T), device=device, dtype=torch.bool))\n",
        "        return mask.view(1, 1, T, T)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        B, T = input_ids.shape\n",
        "        if T > self.max_seq_len:\n",
        "            raise ValueError(f\"Sequence length {T} exceeds max_seq_len={self.max_seq_len}\")\n",
        "\n",
        "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        attn_mask = self._causal_mask(T, input_ids.device)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, attn_mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "def get_shakespeare_slm(\n",
        "    text: str,\n",
        "    max_seq_len: int = 256,\n",
        "    d_model: int = 256,\n",
        "    n_layers: int = 6,\n",
        "    n_heads: int = 8,\n",
        "    d_ff: int = 1024,\n",
        "    dropout: float = 0.1,\n",
        "    seed: int = 42,\n",
        "    device: Optional[str] = None,\n",
        ") -> Tuple[TinyCausalTransformerLM, CharTokenizer]:\n",
        "    # Builds a character tokernizer from the single text and returns a small causal LM\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    tok = build_char_tokenizer_from_text(text)\n",
        "    model = TinyCausalTransformerLM(\n",
        "        vocab_size=len(tok.stoi),\n",
        "        max_seq_len=max_seq_len,\n",
        "        d_model=d_model,\n",
        "        n_layers=n_layers,\n",
        "        n_heads=n_heads,\n",
        "        d_ff=d_ff,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model, tok"
      ],
      "metadata": {
        "id": "4N6qK2fng5PN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model and the tokenizer\n",
        "text = train['text'][0]\n",
        "print(text[:100])\n",
        "model, tok = get_shakespeare_slm(text, max_seq_len=256, d_model=256, n_layers=6, n_heads=8, d_ff=1024)\n",
        "print(\"Vocab size:\", len(tok.stoi))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY7mnD_DiLQ5",
        "outputId": "9cbf291e-cc58-4978-80a3-9fb3e4ec5ae8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Vocab size: 68\n",
            "TinyCausalTransformerLM(\n",
            "  (tok_emb): Embedding(68, 256)\n",
            "  (pos_emb): Embedding(256, 256)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0-5): 6 x TransformerBlock(\n",
            "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): CausalSelfAttention(\n",
            "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
            "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=256, out_features=68, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "nPG8SbL2k0VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomChunkCharDataset(Dataset):\n",
        "    # Get the full tokenized text ids as input and returns a random chunk with length block_size.\n",
        "    def __init__(self, ids: torch.Tensor, block_size: int, num_samples: int, seed: int = 0):\n",
        "        assert ids.dim() == 1\n",
        "        assert block_size >= 2\n",
        "        assert len(ids) > block_size + 1\n",
        "        self.ids = ids\n",
        "        self.block_size = block_size\n",
        "        self.num_samples = num_samples\n",
        "        self.rng = random.Random(seed)\n",
        "        self.max_start = len(ids) - (block_size + 1)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        s = self.rng.randint(0, self.max_start)\n",
        "        chunk = self.ids[s : s + self.block_size + 1]  # [T+1]\n",
        "        x = chunk[:-1].clone()  # [T]\n",
        "        y = chunk[1:].clone()   # [T]\n",
        "        return x, y\n",
        "\n",
        "def _collate_fixed(batch):\n",
        "    x = torch.stack([b[0] for b in batch], dim=0)  # [B,T]\n",
        "    y = torch.stack([b[1] for b in batch], dim=0)  # [B,T]\n",
        "    return x, y\n",
        "\n",
        "def tokenize_text_to_ids(text: str, tok) -> torch.Tensor:\n",
        "    ids = torch.tensor([tok.stoi[c] for c in text], dtype=torch.long)\n",
        "    return ids\n",
        "\n",
        "def split_train_val_ids(ids: torch.Tensor, val_frac: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    n = len(ids)\n",
        "    n_val = max(1, int(n * val_frac))\n",
        "    train_ids = ids[:-n_val]\n",
        "    val_ids = ids[-n_val:]\n",
        "    return train_ids, val_ids\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_lm(model, loader, device) -> float:\n",
        "    model.eval()\n",
        "    total_nll = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(x)  # [B,T,V]\n",
        "        B, T, V = logits.shape\n",
        "        loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T), reduction=\"sum\")\n",
        "\n",
        "        total_nll += loss.item()\n",
        "        total_tokens += B * T\n",
        "\n",
        "    return total_nll / max(1, total_tokens)\n",
        "\n",
        "def train_char_lm(\n",
        "    model,\n",
        "    tok,\n",
        "    text: str,\n",
        "    batch_size: int = 64,\n",
        "    max_seq_len: Optional[int] = None,\n",
        "    epochs: int = 5,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 0.01,\n",
        "    grad_clip: float = 1.0,\n",
        "    val_frac: float = 0.1,\n",
        "    steps_per_epoch: int = 2000,\n",
        "    eval_steps: int = 400,\n",
        "    seed: int = 0,\n",
        "    use_amp: bool = True,\n",
        "    num_workers: int = 0,\n",
        "):\n",
        "    device = next(model.parameters()).device\n",
        "    block_size = max_seq_len if max_seq_len is not None else model.max_seq_len\n",
        "\n",
        "    # tokenize\n",
        "    ids = tokenize_text_to_ids(text, tok)\n",
        "\n",
        "    # split\n",
        "    train_ids, val_ids = split_train_val_ids(ids, val_frac=val_frac)\n",
        "\n",
        "    # datasets (random chunks)\n",
        "    train_ds = RandomChunkCharDataset(\n",
        "        train_ids, block_size=block_size, num_samples=steps_per_epoch * batch_size, seed=seed\n",
        "    )\n",
        "    val_ds = RandomChunkCharDataset(\n",
        "        val_ids, block_size=block_size, num_samples=eval_steps * batch_size, seed=seed + 999\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=_collate_fixed,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=_collate_fixed,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    amp_enabled = bool(use_amp and torch.cuda.is_available())\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=amp_enabled)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_nll = 0.0\n",
        "        running_tokens = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"epoch {ep}/{epochs}\", leave=False)\n",
        "        for x, y in pbar:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "\n",
        "            if amp_enabled:\n",
        "                with torch.amp.autocast(device_type=\"cuda\", enabled=True):\n",
        "                    logits = model(x)\n",
        "                    B, T, V = logits.shape\n",
        "                    loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T), reduction=\"mean\")\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optim)\n",
        "                if grad_clip and grad_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                B, T, V = logits.shape\n",
        "                loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T), reduction=\"mean\")\n",
        "                loss.backward()\n",
        "                if grad_clip and grad_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optim.step()\n",
        "\n",
        "            running_nll += loss.item() * (B * T)\n",
        "            running_tokens += (B * T)\n",
        "\n",
        "            pbar.set_postfix(loss=running_nll / max(1, running_tokens))\n",
        "\n",
        "        train_loss = running_nll / max(1, running_tokens)\n",
        "        val_loss = evaluate_lm(model, val_loader, device=device)\n",
        "\n",
        "        print(f\"[epoch {ep}/{epochs}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "kQB3UqGak01o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_char_lm(\n",
        "    model, tok, text,\n",
        "    batch_size=64,\n",
        "    max_seq_len=256,\n",
        "    epochs=3,\n",
        "    lr=3e-4,\n",
        "    steps_per_epoch=2000,\n",
        "    eval_steps=400,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71,
          "referenced_widgets": [
            "95bcecdb90164097af703e0be284c2fd",
            "24b00ed4177f47ac94952c43364118ca",
            "298aec616f6f4884842bfb369196c527",
            "db7c8b8f148d48be8d29223c82268436",
            "cf55061ef82a4c0b8345687c6b0ba7bc",
            "818f45a51ab44fbaa5a46ab92e32a664",
            "eaea5e7063644da9ba1157299a36b465",
            "ffb5cad5826e4300a6502ad7a9ebe799",
            "4568cb904e7f48d9bc9a0fdbbb894c61",
            "82f6b76ff55248f19dc2caf63a1c579f",
            "48a91591f9554168b905d7e55bfd9223",
            "a0f013b7fe0b4f56a8af18380a88c7fd",
            "f321a4eb09fb4c04ab86d8589e3d6c38",
            "1acf2679e53541fbaf321d1041706aaa",
            "216aa98572064e248a16ec2c697e6ed7",
            "5e0bffb31c9b4f1ba84cf7d1a151e302",
            "5fa8af2f4a6b4694890a534e4646a14c",
            "d8cffa9a2d5e477ebd7a61b7e9cda05d",
            "60d9d9463b8949e890ef4741677e40c2",
            "07a7a6200e6d4e28af1c3727334cc640",
            "5f1639c664e8453680a659c421df49ac",
            "8926168e279e4aefa7877c6dafa17b17",
            "03e1357c907c484bbae229a56aee246c",
            "d00c83d79f8f4c269381c142ce7f5611",
            "65ab43a334c1438a9d9319c1d1536e48",
            "529451864007454baa9897f1963e05c7",
            "ae7375dc6c044abe9bedae8c4a076cf9",
            "e63539dc16564443b62e63c8dd16f0c2",
            "738bc0c031ea421fac4fec1c07db2a4c",
            "d1072da3bbaf4d5a9ee924a6ef0d2930",
            "0af8656e39a040b0a761334c3787b437",
            "4b9971e32031449a88a3fa9f6a60f216",
            "3f2337f7a60b4de7a2105527e1d3c3ca"
          ]
        },
        "id": "Wk2p0Mrck4Eq",
        "outputId": "3fd08650-f311-4fd3-c1d1-43dc83e1cdd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 1/3:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95bcecdb90164097af703e0be284c2fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 1/3] train_loss=1.7894 | val_loss=1.5130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 2/3:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0f013b7fe0b4f56a8af18380a88c7fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 2/3] train_loss=1.2880 | val_loss=1.4490\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 3/3:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03e1357c907c484bbae229a56aee246c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 3/3] train_loss=1.1501 | val_loss=1.4911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate"
      ],
      "metadata": {
        "id": "U8ROB36-qBqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(\n",
        "    model,\n",
        "    tok,\n",
        "    prompt: str = \"\",\n",
        "    max_new_tokens: int = 400,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = None,\n",
        "    do_sample: bool = True,\n",
        ") -> str:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    if prompt:\n",
        "        input_ids = torch.tensor([tok.stoi[c] for c in prompt], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    else:\n",
        "        start_char = \"\\n\" if \"\\n\" in tok.stoi else \" \"\n",
        "        input_ids = torch.tensor([tok.stoi[start_char]], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate token in each timesteps\n",
        "    for _ in range(max_new_tokens):\n",
        "        if input_ids.size(1) > model.max_seq_len:\n",
        "            input_ids = input_ids[:, -model.max_seq_len :]\n",
        "\n",
        "        logits = model(input_ids)[:, -1, :]  # [1,1,V]\n",
        "        logits = logits / max(1e-8, temperature)\n",
        "\n",
        "        if top_k is not None and top_k > 0:\n",
        "            k = min(top_k, logits.size(-1))\n",
        "            v, _ = torch.topk(logits, k=k)\n",
        "            logits = torch.where(logits < v[:, [-1]], torch.full_like(logits, float(\"-inf\")), logits)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        if do_sample:\n",
        "            next_id = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
        "        else:\n",
        "            next_id = torch.argmax(probs, dim=-1, keepdim=True)  # [1,1]\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
        "\n",
        "    return \"\".join(tok.itos[int(i)] for i in input_ids.squeeze(0).tolist())"
      ],
      "metadata": {
        "id": "dPQQly9rqCXg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 500 texts with trained LM.\n",
        "# This texts are used for organizing preference dataset for finetuning.\n",
        "\n",
        "generated_text_list = []\n",
        "for _ in tqdm(range(500)):\n",
        "    temp = generate_text(model, tok, prompt=\"ROMEO:\\n\", max_new_tokens=400, temperature=0.9, top_k=10, do_sample=True)\n",
        "    generated_text_list.append(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "999cd126876e42c79664782a2eca3317",
            "cc846848fb864f4fa75d8610dbfdddb5",
            "a3d17a9b8d2d4ecc9ea4f161abb7344d",
            "f004fa290b7444c18373ebc40ce93843",
            "be680cf0488443e8905288934941a763",
            "6706d319a35043aab2959b2b127d8ef9",
            "a6454bffd4254ce58abef02b2f564d9e",
            "bd908feeb7134a87acb442fc4a67617c",
            "9ed020702a1c434ab76f514f24f2387f",
            "500ee3e611704ab7b672585c77ea6d4d",
            "fe9782e994bb4582916975ed171e79d4"
          ]
        },
        "id": "Twb2bw81tOuH",
        "outputId": "70cc6e65-a1e6-405d-d685-fa2750ba18db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "999cd126876e42c79664782a2eca3317"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset for Finetuning"
      ],
      "metadata": {
        "id": "xZGJFTAz-Mtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def anti_repetition_score(\n",
        "    text: str,\n",
        "    unit: Literal[\"word\", \"char_ngram\", \"word_ngram\"] = \"word\",\n",
        "    max_repeats: int = 1,\n",
        "    n: int = 3,\n",
        "    normalize_case: bool = True,\n",
        "    ignore_short_words_len_lt: int = 2,\n",
        "    penalty_power: float = 4.0,\n",
        "    clamp_top_k: int = 0,\n",
        "    return_details: bool = False,\n",
        ") -> float | Tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    This function returns score with the given text. Returned score has the value in [0,1].\n",
        "    If the text does not have any repeated words, it will get the score 1.\n",
        "    The more repeated words, the lower the score will be.\n",
        "\n",
        "    Arguments:\n",
        "        text (str) : Text to score.\n",
        "        unit (Literal) : How to partition the text.\n",
        "        max_repeats (int) : Maximum number of repeats allowed without any penalty.\n",
        "        n (int) : Number of character or word if the unit is char_ngram or word_ngram.\n",
        "        normalize_case (bool) : Lower the text or not.\n",
        "        ignore_short_words_len_lt (int) : If the word is less than this value,\n",
        "                                        then the repetition of the word is allowed without any penalty.\n",
        "        penalty_power (float) : Larger penalty power makes the penalty score higher and final score lower.\n",
        "        clamp_top_k (int) : If > 0, only the top-k most repeated units contribute to the penalty.\n",
        "        return_details (bool) : Return details or not.\n",
        "\n",
        "    Returns:\n",
        "        float : Score.\n",
        "        dict : Details. (optional)\n",
        "\n",
        "    \"\"\"\n",
        "    # Normalize\n",
        "    if normalize_case:\n",
        "        text_proc = text.lower()\n",
        "    else:\n",
        "        text_proc = text\n",
        "\n",
        "    # Check if the value is appropriate\n",
        "    if max_repeats < 1:\n",
        "        raise ValueError(\"max_repeats must be >= 1\")\n",
        "    if unit in (\"char_ngram\", \"word_ngram\") and n < 1:\n",
        "        raise ValueError(\"n must be >= 1 for n-gram modes\")\n",
        "\n",
        "    # Build units\n",
        "    if unit == \"word\":\n",
        "        words = _WORD_RE.findall(text_proc)\n",
        "        if ignore_short_words_len_lt > 0:\n",
        "            words = [w for w in words if len(w) >= ignore_short_words_len_lt]\n",
        "        units = words\n",
        "\n",
        "    elif unit == \"char_ngram\":\n",
        "        s = text_proc\n",
        "        units = [s[i:i+n] for i in range(max(0, len(s) - n + 1))]\n",
        "\n",
        "    elif unit == \"word_ngram\":\n",
        "        words = _WORD_RE.findall(text_proc)\n",
        "        if ignore_short_words_len_lt > 0:\n",
        "            words = [w for w in words if len(w) >= ignore_short_words_len_lt]\n",
        "        units = [\" \".join(words[i:i+n]) for i in range(max(0, len(words) - n + 1))]\n",
        "    else:\n",
        "        raise ValueError(\"unit must be one of: 'word', 'char_ngram', 'word_ngram'\")\n",
        "\n",
        "    # Return if there is no unit\n",
        "    total = len(units)\n",
        "    if total == 0:\n",
        "        score = 1.0\n",
        "        details = {\"total_units\": 0, \"penalty\": 0.0, \"top_repeats\": []}\n",
        "        return (score, details) if return_details else score\n",
        "\n",
        "    counts = Counter(units)\n",
        "\n",
        "    # Count excess per unit (beyond max_repeats)\n",
        "    excess_per_unit = [(u, max(0, c - max_repeats), c) for u, c in counts.items()]\n",
        "    excess_per_unit.sort(key=lambda t: t[1], reverse=True)\n",
        "\n",
        "    if clamp_top_k and clamp_top_k > 0:\n",
        "        excess_per_unit = excess_per_unit[:clamp_top_k]\n",
        "\n",
        "    # Calculate the penalty\n",
        "    penalty = 0.0\n",
        "    for _, ex, _ in excess_per_unit:\n",
        "        if ex > 0:\n",
        "            penalty += (ex ** penalty_power)\n",
        "\n",
        "    # Normalize the penalty by length\n",
        "    penalty = penalty / total\n",
        "    score = math.exp(-penalty)\n",
        "\n",
        "    if return_details:\n",
        "        top = counts.most_common(10)\n",
        "        details = {\n",
        "            \"total_units\": total,\n",
        "            \"unique_units\": len(counts),\n",
        "            \"max_repeats\": max_repeats,\n",
        "            \"penalty_power\": penalty_power,\n",
        "            \"penalty\": penalty,\n",
        "            \"top_repeats\": top,\n",
        "        }\n",
        "        return score, details\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "RROZPZTC-N_I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_preference_dataset_from_texts(\n",
        "    prompts: List[str],\n",
        "    generated_texts: List[str],\n",
        "    score_fn: Callable[[str], float],\n",
        "    n_pairs: int = 5000,\n",
        "    min_y_chars: int = 120,\n",
        "    max_y_chars: int = 300,\n",
        "    min_score_gap: float = 0.10,\n",
        "    seed: int = 0,\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    This function organizes preference dataset using the reward function and generated texts.\n",
        "\n",
        "    Arguments:\n",
        "        prompts (List) : List of input prompts(x).\n",
        "        generated_texts (List) : List of generated texts(y).\n",
        "        score_fn (Callable) : Reward function.\n",
        "        n_pairs (int) : Number of preference pairs to be contained in the preference dataset.\n",
        "        min_y_chars (int) : Minimum number of characters in y.\n",
        "        max_y_chars (int) : Maximum number of characters in y.\n",
        "        min_score_gap (float) : Minimum score gap between two samples.\n",
        "        seed (int) : Random seed.\n",
        "\n",
        "    Returns:\n",
        "        List : List of preference pairs (x,y_w,y_l).\n",
        "    \"\"\"\n",
        "    # Check if the arguments are appropriate\n",
        "    assert len(prompts) == len(generated_texts)\n",
        "    assert n_pairs > 0\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    # by_prompt saves the data of the form : {x : [y, score]}\n",
        "    by_prompt: Dict[str, List[Tuple[str, float]]] = {}\n",
        "\n",
        "    for x, full in zip(prompts, generated_texts):\n",
        "        # Strip x if full starts with x\n",
        "        y = full[len(x):] if full.startswith(x) else full\n",
        "\n",
        "        # Refine y\n",
        "        y = y.strip(\"\\n\")\n",
        "        if len(y) < min_y_chars:\n",
        "            continue\n",
        "        if len(y) > max_y_chars:\n",
        "            y = y[:max_y_chars]\n",
        "\n",
        "        # Compute the score(reward) of y\n",
        "        s = float(score_fn(y))\n",
        "\n",
        "        # Update by_prompt\n",
        "        by_prompt.setdefault(x, []).append((y, s))\n",
        "\n",
        "    # For each prompt, sort by score and keep as candidates\n",
        "    for x in list(by_prompt.keys()):\n",
        "        cands = by_prompt[x]\n",
        "        if len(cands) < 2:\n",
        "            del by_prompt[x]\n",
        "            continue\n",
        "        cands.sort(key=lambda t: t[1])\n",
        "        by_prompt[x] = cands\n",
        "\n",
        "    if not by_prompt:\n",
        "        raise ValueError(\"No prompts have >=2 usable candidates after filtering. \"\n",
        "                         \"Try lowering min_y_chars or max_y_chars, or provide more samples.\")\n",
        "\n",
        "    prompt_keys = list(by_prompt.keys())\n",
        "\n",
        "    pref_data: List[Dict[str, str]] = []\n",
        "\n",
        "    # Sample a winner/loser from one prompt with a minimum score gap\n",
        "    def sample_pair_for_prompt(x: str) -> Optional[Dict[str, str]]:\n",
        "        cands = by_prompt[x]\n",
        "        for _ in range(12):\n",
        "            i, j = rng.sample(range(len(cands)), 2)\n",
        "            (y1, s1), (y2, s2) = cands[i], cands[j]\n",
        "            if abs(s1 - s2) >= min_score_gap:\n",
        "                if s1 > s2:\n",
        "                    return {\"x\": x, \"y_w\": y1, \"y_l\": y2}\n",
        "                else:\n",
        "                    return {\"x\": x, \"y_w\": y2, \"y_l\": y1}\n",
        "\n",
        "        (y_low, s_low) = cands[0]\n",
        "        (y_high, s_high) = cands[-1]\n",
        "        if (s_high - s_low) >= min_score_gap:\n",
        "            return {\"x\": x, \"y_w\": y_high, \"y_l\": y_low}\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Build dataset\n",
        "    max_trials = n_pairs * 10\n",
        "    trials = 0\n",
        "    while len(pref_data) < n_pairs and trials < max_trials:\n",
        "        trials += 1\n",
        "        x = rng.choice(prompt_keys)\n",
        "        pair = sample_pair_for_prompt(x)\n",
        "        if pair is None:\n",
        "            continue\n",
        "        if pair[\"y_w\"] == pair[\"y_l\"]:\n",
        "            continue\n",
        "\n",
        "        pref_data.append(pair)\n",
        "\n",
        "    if len(pref_data) < n_pairs:\n",
        "        print(f\"[WARN] Only built {len(pref_data)} / {n_pairs} pairs. \"\n",
        "              f\"Try lowering min_score_gap (currently {min_score_gap}) or relaxing length filters.\")\n",
        "\n",
        "    return pref_data"
      ],
      "metadata": {
        "id": "Ul1wZxKc-Rw_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"ROMEO:\\n\"] * 500\n",
        "pref_data = build_preference_dataset_from_texts(\n",
        "    prompts, generated_text_list,\n",
        "    score_fn=anti_repetition_score,\n",
        "    n_pairs=3000,\n",
        "    min_y_chars=120,\n",
        "    max_y_chars=280,\n",
        "    min_score_gap=0.12,\n",
        "    seed=0,\n",
        ")\n",
        "print(len(pref_data))\n",
        "print(pref_data[0].keys())\n",
        "print(pref_data[0][\"x\"][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "labtS5Pp-YyB",
        "outputId": "b1d2b028-b6b3-49ef-f259-88c7e6a39b0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "dict_keys(['x', 'y_w', 'y_l'])\n",
            "ROMEO:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DPO"
      ],
      "metadata": {
        "id": "h2-STEAVCpyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharPreferenceDataset(Dataset):\n",
        "    # Get pref_data of the form [{x, y_w, y_l}]\n",
        "    # Returns {w_ids, w_mask, l_ids, l_mask}\n",
        "    def __init__(\n",
        "        self,\n",
        "        pref_data: List[Dict[str, str]],\n",
        "        tok,\n",
        "        max_seq_len: int,\n",
        "        add_eos_to_y: bool = False,\n",
        "    ):\n",
        "        self.data = pref_data\n",
        "        self.tok = tok\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.add_eos_to_y = add_eos_to_y\n",
        "\n",
        "        # Indices of valid data in pref_data are stored in valid_idx\n",
        "        self.valid_idx = []\n",
        "        for i, ex in enumerate(self.data):\n",
        "            x, yw, yl = ex[\"x\"], ex[\"y_w\"], ex[\"y_l\"]\n",
        "            if all(c in tok.stoi for c in x) and all(c in tok.stoi for c in yw) and all(c in tok.stoi for c in yl):\n",
        "                self.valid_idx.append(i)\n",
        "\n",
        "        if len(self.valid_idx) == 0:\n",
        "            raise ValueError(\"No valid examples: found OOV characters w.r.t. tokenizer vocab.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.valid_idx)\n",
        "\n",
        "    def _encode_xy(self, x: str, y: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Encode x+y as raw stream\n",
        "        x_ids = [self.tok.stoi[c] for c in x]\n",
        "        y_ids = [self.tok.stoi[c] for c in y]\n",
        "        if self.add_eos_to_y:\n",
        "            y_ids = y_ids + [self.tok.eos_id]\n",
        "\n",
        "        ids = x_ids + y_ids\n",
        "        y_start = len(x_ids)\n",
        "        y_mask = [0] * y_start + [1] * (len(ids) - y_start)\n",
        "\n",
        "        # Truncate from the left so we preserve the end of y if too long\n",
        "        if len(ids) > self.max_seq_len:\n",
        "            overflow = len(ids) - self.max_seq_len\n",
        "            ids = ids[overflow:]\n",
        "            y_mask = y_mask[overflow:]\n",
        "\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(y_mask, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Returns ids, mask of x+y\n",
        "        ex = self.data[self.valid_idx[idx]]\n",
        "        x, yw, yl = ex[\"x\"], ex[\"y_w\"], ex[\"y_l\"]\n",
        "        w_ids, w_mask = self._encode_xy(x, yw)\n",
        "        l_ids, l_mask = self._encode_xy(x, yl)\n",
        "        return {\"w_ids\": w_ids, \"w_mask\": w_mask, \"l_ids\": l_ids, \"l_mask\": l_mask}\n",
        "\n",
        "def collate_char_pref(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "    # Batch collator. Merge the dictionaries and add attention mask.\n",
        "    B = len(batch)\n",
        "    w_max = max(b[\"w_ids\"].size(0) for b in batch)\n",
        "    l_max = max(b[\"l_ids\"].size(0) for b in batch)\n",
        "\n",
        "    w_ids = torch.full((B, w_max), 0, dtype=torch.long)\n",
        "    w_mask = torch.zeros((B, w_max), dtype=torch.float32)\n",
        "    w_attn = torch.zeros((B, w_max), dtype=torch.float32)\n",
        "\n",
        "    l_ids = torch.full((B, l_max), 0, dtype=torch.long)\n",
        "    l_mask = torch.zeros((B, l_max), dtype=torch.float32)\n",
        "    l_attn = torch.zeros((B, l_max), dtype=torch.float32)\n",
        "\n",
        "    for i, b in enumerate(batch):\n",
        "        wl = b[\"w_ids\"].size(0)\n",
        "        ll = b[\"l_ids\"].size(0)\n",
        "        w_ids[i, :wl] = b[\"w_ids\"]\n",
        "        w_mask[i, :wl] = b[\"w_mask\"]\n",
        "        w_attn[i, :wl] = 1.0\n",
        "        l_ids[i, :ll] = b[\"l_ids\"]\n",
        "        l_mask[i, :ll] = b[\"l_mask\"]\n",
        "        l_attn[i, :ll] = 1.0\n",
        "\n",
        "    return {\n",
        "        \"w_ids\": w_ids, \"w_mask\": w_mask, \"w_attn\": w_attn,\n",
        "        \"l_ids\": l_ids, \"l_mask\": l_mask, \"l_attn\": l_attn,\n",
        "    }\n",
        "\n",
        "def sum_logp_y_region(\n",
        "    model,\n",
        "    input_ids: torch.Tensor,\n",
        "    y_mask: torch.Tensor,\n",
        "    attn_mask: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute summation of log likelihood of the predicted y tokens.\n",
        "    Sum log likelihood : log pi_theta(y|x)\n",
        "    This function is used in DPO objective.\n",
        "\n",
        "    Arguments:\n",
        "        model (nn.Module) : Language model.\n",
        "        input_ids (torch.Tensor) : [B,T]\n",
        "        y_mask (torch.Tensor) : [B,T]\n",
        "        attn_mask (torch.Tensor) : [B,T]\n",
        "\n",
        "    Returns:\n",
        "        sum_logp (torch.Tensor) : [B] sum log p(y|x) over predicted y tokens\n",
        "        y_len (torch.Tensor) : [B] number of predicted y tokens used\n",
        "    \"\"\"\n",
        "    logits = model(input_ids) # [B,T,V]\n",
        "    logp = F.log_softmax(logits, dim=-1) # [B,T,V]\n",
        "\n",
        "    targets = input_ids[:, 1:] # [B,T-1]\n",
        "    tok_logp = logp[:, :-1].gather(-1, targets.unsqueeze(-1)).squeeze(-1) # [B,T-1]\n",
        "\n",
        "    mask = (y_mask[:, 1:] * attn_mask[:, 1:]) # [B,T-1]\n",
        "    y_len = mask.sum(dim=-1).clamp_min(1.0) # [B]\n",
        "    sum_logp = (tok_logp * mask).sum(dim=-1) # [B]\n",
        "    return sum_logp, y_len\n",
        "\n",
        "def finetune_dpo_char_lm(\n",
        "    model,\n",
        "    tok,\n",
        "    pref_data: List[Dict[str, str]],\n",
        "    max_seq_len: Optional[int] = None,\n",
        "    add_eos_to_y: bool = False,\n",
        "    batch_size: int = 64,\n",
        "    epochs: int = 3,\n",
        "    lr: float = 1e-4,\n",
        "    weight_decay: float = 0.01,\n",
        "    grad_clip: float = 1.0,\n",
        "    beta: float = 0.1,\n",
        "    val_split: float = 0.05,\n",
        "    seed: int = 0,\n",
        "    use_amp: bool = True,\n",
        "    num_workers: int = 0,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    DPO objective (standard):\n",
        "      delta = (logπθ(yw|x) - logπθ(yl|x)) - (logπref(yw|x) - logπref(yl|x))\n",
        "      loss  = -logsigmoid(beta * delta)\n",
        "\n",
        "    We use SUM logprobs over y tokens (common for DPO).\n",
        "    Reference model = frozen copy of model at start of DPO finetuning.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    if max_seq_len is None:\n",
        "        max_seq_len = model.max_seq_len\n",
        "\n",
        "    # Freeze the reference model\n",
        "    ref_model = type(model)(\n",
        "        vocab_size=model.vocab_size,\n",
        "        max_seq_len=model.max_seq_len,\n",
        "        d_model=model.tok_emb.embedding_dim,\n",
        "        n_layers=len(model.blocks),\n",
        "        n_heads=model.blocks[0].attn.n_heads,\n",
        "        d_ff=model.blocks[0].ff[0].out_features,\n",
        "        dropout=0.0,\n",
        "    ).to(device)\n",
        "    ref_model.load_state_dict(model.state_dict())\n",
        "    ref_model.eval()\n",
        "    for p in ref_model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    # Split preference data\n",
        "    idxs = list(range(len(pref_data)))\n",
        "    random.shuffle(idxs)\n",
        "    n_val = max(1, int(len(idxs) * val_split))\n",
        "    val_idxs = idxs[:n_val]\n",
        "    tr_idxs = idxs[n_val:]\n",
        "\n",
        "    # Get dataset\n",
        "    train_list = [pref_data[i] for i in tr_idxs]\n",
        "    val_list = [pref_data[i] for i in val_idxs]\n",
        "\n",
        "    train_ds = CharPreferenceDataset(train_list, tok, max_seq_len=max_seq_len, add_eos_to_y=add_eos_to_y)\n",
        "    val_ds = CharPreferenceDataset(val_list, tok, max_seq_len=max_seq_len, add_eos_to_y=add_eos_to_y)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_char_pref,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=False,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_char_pref,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    amp_enabled = bool(use_amp and torch.cuda.is_available())\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=amp_enabled)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_val() -> Tuple[float, float]:\n",
        "        # Returns loss and win rate of the val dataset\n",
        "        # \"Win\" means log(pi_theta(y_w|x) / pi_ref(y_w|x)) > log(pi_theta(y_l|x) / pi_ref(y_l|x))\n",
        "        # which is equal to delta > 0\n",
        "        model.eval()\n",
        "        tot_loss = 0.0\n",
        "        tot_n = 0\n",
        "        tot_win = 0\n",
        "        for batch in val_loader:\n",
        "            w_ids = batch[\"w_ids\"].to(device)\n",
        "            w_mask = batch[\"w_mask\"].to(device)\n",
        "            w_attn = batch[\"w_attn\"].to(device)\n",
        "\n",
        "            l_ids = batch[\"l_ids\"].to(device)\n",
        "            l_mask = batch[\"l_mask\"].to(device)\n",
        "            l_attn = batch[\"l_attn\"].to(device)\n",
        "\n",
        "            # policy\n",
        "            w_sum, _ = sum_logp_y_region(model, w_ids, w_mask, w_attn)\n",
        "            l_sum, _ = sum_logp_y_region(model, l_ids, l_mask, l_attn)\n",
        "\n",
        "            # reference\n",
        "            rw_sum, _ = sum_logp_y_region(ref_model, w_ids, w_mask, w_attn)\n",
        "            rl_sum, _ = sum_logp_y_region(ref_model, l_ids, l_mask, l_attn)\n",
        "\n",
        "            delta = (w_sum - l_sum) - (rw_sum - rl_sum)  # [B]\n",
        "            loss = -F.logsigmoid(beta * delta).mean()\n",
        "\n",
        "            tot_loss += loss.item() * w_ids.size(0)\n",
        "            tot_win += (delta > 0).sum().item()\n",
        "            tot_n += w_ids.size(0)\n",
        "\n",
        "        return tot_loss / max(1, tot_n), tot_win / max(1, tot_n)\n",
        "\n",
        "    # Train\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        seen = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"DPO ep {ep}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            w_ids = batch[\"w_ids\"].to(device)\n",
        "            w_mask = batch[\"w_mask\"].to(device)\n",
        "            w_attn = batch[\"w_attn\"].to(device)\n",
        "\n",
        "            l_ids = batch[\"l_ids\"].to(device)\n",
        "            l_mask = batch[\"l_mask\"].to(device)\n",
        "            l_attn = batch[\"l_attn\"].to(device)\n",
        "\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "\n",
        "            if amp_enabled:\n",
        "                with torch.amp.autocast(device_type=\"cuda\", enabled=True):\n",
        "                    w_sum, _ = sum_logp_y_region(model, w_ids, w_mask, w_attn)\n",
        "                    l_sum, _ = sum_logp_y_region(model, l_ids, l_mask, l_attn)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        rw_sum, _ = sum_logp_y_region(ref_model, w_ids, w_mask, w_attn)\n",
        "                        rl_sum, _ = sum_logp_y_region(ref_model, l_ids, l_mask, l_attn)\n",
        "\n",
        "                    delta = (w_sum - l_sum) - (rw_sum - rl_sum)\n",
        "                    loss = -F.logsigmoid(beta * delta).mean()\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optim)\n",
        "                if grad_clip and grad_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                w_sum, _ = sum_logp_y_region(model, w_ids, w_mask, w_attn)\n",
        "                l_sum, _ = sum_logp_y_region(model, l_ids, l_mask, l_attn)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    rw_sum, _ = sum_logp_y_region(ref_model, w_ids, w_mask, w_attn)\n",
        "                    rl_sum, _ = sum_logp_y_region(ref_model, l_ids, l_mask, l_attn)\n",
        "\n",
        "                delta = (w_sum - l_sum) - (rw_sum - rl_sum)\n",
        "                loss = -F.logsigmoid(beta * delta).mean()\n",
        "\n",
        "                loss.backward()\n",
        "                if grad_clip and grad_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optim.step()\n",
        "\n",
        "            running_loss += loss.item() * w_ids.size(0)\n",
        "            seen += w_ids.size(0)\n",
        "            pbar.set_postfix(loss=running_loss / max(1, seen))\n",
        "\n",
        "        train_loss = running_loss / max(1, seen)\n",
        "        val_loss, val_winrate = eval_val()\n",
        "        print(f\"[DPO ep {ep}/{epochs}] train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_winrate={val_winrate:.3f}\")"
      ],
      "metadata": {
        "id": "zn8hQo4bFbtV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_dpo_char_lm(\n",
        "    model, tok, pref_data,\n",
        "    max_seq_len=256,\n",
        "    batch_size=64,\n",
        "    epochs=3,\n",
        "    lr=1e-4,\n",
        "    beta=0.1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167,
          "referenced_widgets": [
            "2f300d1cce8240ed817ba4e461b6fef7",
            "dcb3991f186e4261a68265a0b40fa188",
            "f0b3b004d57e4bc4bc77128efc60c682",
            "3ef38c42f9f4457b9c606d61ca49a9b3",
            "07646de7e65949bfa4d347bb4849b595",
            "d0b84c19863e49599b0f6542f562c61e",
            "a3259d966b3c4d7eabb7047292f5e7c2",
            "b3e072add260424d85707344872bd90c",
            "b99fb8d870ff4ce590fd31465400bade",
            "546474484db54200adea1dd7616a6ad2",
            "42ab39fbec4c45ddb60b11d657372f21",
            "577ce897fdb74bb59f67dcdf72b77709",
            "ab456a27d26f41e0adeed4d234df492e",
            "1024493f98524828af8d1fc2d57c2d74",
            "39ea726c88154bcb8d4f58a4d5ed5283",
            "7b85a7422704444ea970a980af2cdd85",
            "d0ed0e29832c4e64a6fe302bda92c98a",
            "7f8b5685c9ea4010812ebb93cfeae018",
            "1199998327254827b22f84b14287950d",
            "1f2365bd91914e1292a0d7e52a802917",
            "7e96adbd9ef74269bfa5f5e0de7bd242",
            "402fbc7f05e046738f2aa1eee671527f",
            "6ba78d65bc6a4e109a93d7a0beb8d73b",
            "82f23c552dd44ed598375d83e0639203",
            "d5a41ed86a4a4fc8966016a9805377c5",
            "690a89f110fe481f9bfd203d2d3bde58",
            "66ee562662e2482da2ed0f8ff3638c32",
            "85775f0b75294a5596b5fdbb85330abb",
            "10b85d4708e040ecac891c6ff3ef586c",
            "c7e3f79b5aea47d59a5d8ece87a7dbd7",
            "4aeb30edc0b1471db5624af698484016",
            "f0150ac5e0954e89aae78b9c7c7cab8a",
            "f582b6f483de4e828a5c969d307ac432"
          ]
        },
        "id": "NnS9ouy7FqDR",
        "outputId": "8e913a55-7c08-4158-f335-e213242f3021"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DPO ep 1/3:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f300d1cce8240ed817ba4e461b6fef7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DPO ep 1/3] train_loss=0.3730  val_loss=0.1195  val_winrate=0.947\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DPO ep 2/3:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "577ce897fdb74bb59f67dcdf72b77709"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DPO ep 2/3] train_loss=0.1144  val_loss=0.0579  val_winrate=0.987\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DPO ep 3/3:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ba78d65bc6a4e109a93d7a0beb8d73b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DPO ep 3/3] train_loss=0.0814  val_loss=0.0265  val_winrate=0.993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "rUWp0krKXYC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of texts to evalulate the result\n",
        "n = 50\n",
        "\n",
        "# We already have list of texts generated with reference model\n",
        "ref_texts = generated_text_list[:n]\n",
        "\n",
        "# List of texts generated with DPO-finetuned model\n",
        "dpo_texts = []\n",
        "for _ in tqdm(range(n)):\n",
        "    temp = generate_text(model, tok, prompt=\"ROMEO:\\n\", max_new_tokens=400, temperature=0.9, top_k=10, do_sample=True)\n",
        "    dpo_texts.append(temp)\n",
        "\n",
        "# Compare the score\n",
        "ref_scores = [anti_repetition_score(text) for text in ref_texts]\n",
        "dpo_scores = [anti_repetition_score(text) for text in dpo_texts]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(ref_scores, bins=10, alpha=0.5, label='Reference Model')\n",
        "plt.hist(dpo_scores, bins=10, alpha=0.5, label='DPO-Finetuned Model')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Scores')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596,
          "referenced_widgets": [
            "ed237124a7404697b041beb12e2beddd",
            "2ba4ed7ef5d6436b88f474030307f7dc",
            "84973500151d4f4c8b264534d0e750b9",
            "61696d19f9cb47949adfcc3fa984c922",
            "28f1f1b00916410c80f94965cbea60a2",
            "bb34fb6b959b417d8533db8bb0c1999a",
            "d6d8c1e0270f4b6cae6568e2fcd9ea93",
            "471e7cc4e5394d1498ed60a17b4a2044",
            "82aa987a058d40f4af3dc748ee15f06e",
            "c88aff3f113f414c86d58f908078fe84",
            "ed144c972af24dcdaa63922143be193b"
          ]
        },
        "id": "3iI0sBhuXZym",
        "outputId": "aef4e239-1195-472d-a8d1-f9d7fcea6691"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed237124a7404697b041beb12e2beddd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the generated text is not abnormal\n",
        "print(\"Reference model genrates:\")\n",
        "print(generated_text_list[1])\n",
        "print(\"-\"*50)\n",
        "print(\"DPO-finetuned model genrates:\")\n",
        "print(dpo_texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw01uZmVKvX8",
        "outputId": "48483289-a7e0-4396-ac70-b74259166a44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference model genrates:\n",
            "y be; 'tis some mock spirit of accuses.\n",
            "\n",
            "MENENIUS:\n",
            "You have made themselves. I, sir, good Capitol,\n",
            "Who, good man consent years, still on, have you lived\n",
            "By mother, which way to call your chance\n",
            "A worthy gods, how with the great curboard!\n",
            "\n",
            "SICINIUS:\n",
            "Then, we\n",
            "--------------------------------------------------\n",
            "DPO-finetuned model genrates:\n",
            " dragon?\n",
            "What, lord me dotht? I here death in his death\n",
            "Where never come be some that have been so did dealiner,\n",
            "Whose were have they done she't.\n",
            "I will not we appeared this: here is burins.\n",
            "I am courter'd breath, how not I will'd our chare,\n",
            "Lest I were a l\n"
          ]
        }
      ]
    }
  ]
}
