{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "K9gxl4MBvLHj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Paper : \"Score-based Generative Modeling through Stochastic Differential Equations\" by Yang Song et al. (2021)\n",
        "# The code below was written with reference to the paper's official open source code.\n",
        "# Github Repository : https://github.com/yang-song/score_sde"
      ],
      "metadata": {
        "id": "OU3gWP8vuxUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "jdpCW7YevJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import math"
      ],
      "metadata": {
        "id": "DHIEBJDedkwg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "K9gxl4MBvLHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Gaussian Fourier embeddings for continuous noise levels.\n",
        "\n",
        "    Parameters:\n",
        "        embedding_size (int) : Size of embedding\n",
        "        scale (float) : Scaling factor\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data. Get timestep t from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Gaussian Fourier embeddings\n",
        "\n",
        "        Example:\n",
        "            >>> embedding_size = 64\n",
        "            >>> scale = 1.0\n",
        "            >>> fourier = GaussianFourierProjection(embedding_size, scale)\n",
        "            >>> t = torch.randn(64)\n",
        "            >>> out = fourier(t)\n",
        "            >>> print(out.shape) # torch.Size([64,128])\n",
        "        \"\"\"\n",
        "        x_proj = x.unsqueeze(1) * self.W.unsqueeze(0) * 2 * math.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "def swish(x):\n",
        "    # Activation function used in DDPM\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    # GroupNorm 8 is used in the model instead of GroupNorm 32\n",
        "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    # 1x1 convolution\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    # Block that doubles down on resolution. Use convolution block or average pooling block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    # Block that doubles the resolution. Use convolution block or interpolating block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet block used in DDPM. Use group normalization. Continuous timestep t is also received as input.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of Output channels\n",
        "        temb_channels (int) : Number of time embedding channels\n",
        "        dropout (float) : Dropout rate\n",
        "        conv_shortcut (bool) : Whether to add convolution shortcut\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None,\n",
        "                 temb_channels=512, dropout=0.0, conv_shortcut=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = GroupNorm32(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm32(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.conv_shortcut:\n",
        "                self.conv_shortcut = nn.Conv2d(self.in_channels, self.out_channels,\n",
        "                                               kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "        else:\n",
        "            self.conv_shortcut = None\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            temb (torch.Tensor) : Embedding vector of timestep t\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,3,32,32)\n",
        "            >>> temb = torch.randn(128,64)\n",
        "            >>> block = ResnetBlock(in_channels=3,out_channels=32,temb_channels=64)\n",
        "            >>> out = block(x,temb)\n",
        "            >>> print(out.shape) # torch.Size([128,32,32,32])\n",
        "        \"\"\"\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h) # B*3*32*32\n",
        "        h = self.conv1(h) # B*32*32*32\n",
        "        h_temb = swish(temb) # B*64\n",
        "        h_temb = self.temb_proj(h_temb)  # B*32\n",
        "        h_temb = h_temb[:, :, None, None]  # B*32*1*1\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h) # B*32*32*32\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h) # B*32*32*32\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x) # B*32*32*32\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention block used in DDPM.\n",
        "\n",
        "    Parameters:\n",
        "        channels (int) : Number of input, output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm32(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,32,224,224)\n",
        "            >>> block = AttnBlock(32)\n",
        "            >>> out = block(x)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h).permute(0, 2, 3, 1)  # (B, H, W, C)\n",
        "        k = self.k(h).permute(0, 2, 3, 1)\n",
        "        v = self.v(h).permute(0, 2, 3, 1)\n",
        "        w = torch.einsum('bhwc,bHWc->bhwHW', q, k) * (C ** -0.5)\n",
        "        w = w.reshape(B, H, W, H * W)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = w.reshape(B, H, W, H, W)\n",
        "        h_ = torch.einsum('bhwHW,bHWc->bhwc', w, v)\n",
        "        h_ = self.proj_out(h_.permute(0, 3, 1, 2))\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Final DDPMModel. Modified version of UNet.\n",
        "    Attention block is applied to where it is set to applied.\n",
        "    Given noised data x_t and timestep t, the model estimates the value of noise at timestep t.\n",
        "    This DDPM model is same as the one used in the DDPM paper except for the GaussianFourierProjection.\n",
        "    Unlike origianl DDPM model, timestep t is set to [0,1] in this model.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of output channels\n",
        "        ch (int) : Number of default channel\n",
        "        ch_mult (tuple) : Coefficient multiblied by channels\n",
        "        num_res_blocks (int) : Number of resnet blocks\n",
        "        attn_resolutions (set) : Set of resolutions at which attention block applies\n",
        "        dropout (float) : Dropout rate\n",
        "        resamp_with_conv (bool) : Whether to use convolution while down(up)sampling\n",
        "        init_resolution (int) : Resolution of input images\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 2, 4),\n",
        "        num_res_blocks=3,\n",
        "        attn_resolutions={28,56},\n",
        "        dropout=0.1,\n",
        "        resamp_with_conv=True,\n",
        "        init_resolution=224\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "\n",
        "        # Time Embedding\n",
        "        self.fourier = GaussianFourierProjection(embedding_size=self.ch, scale=1.0)\n",
        "\n",
        "        # Dimension of time embedding vector\n",
        "        self.temb_ch = ch * 4\n",
        "\n",
        "        # Timestep embedding layers\n",
        "        self.temb_dense0 = linear(2*self.ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "\n",
        "        # Input conv\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Downsample blocks\n",
        "        # Each downsampling block(modulelist) is stored in down_blocks.\n",
        "        # Each block is composed of resnetblocks and attention block(if needed).\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        # Symmetric structure with downsample blocks\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "\n",
        "        # output conv\n",
        "        self.norm_out = GroupNorm32(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            sigma (torch.Tensor) : Timesteps of batch data. Get continuous noise level sigma(t) from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            x = torch.randn(128,3,32,32)\n",
        "            t = torch.rand(128)\n",
        "            model = DDPMModel(ch=64,attn_resolution={16},init_resolution=32)\n",
        "            out = model(x,t)\n",
        "            print(out.shape) # torch.Size([128,3,32,32])\n",
        "        \"\"\"\n",
        "        # 1) Timestep embedding\n",
        "        log_sigma = torch.log(sigma)\n",
        "        temb = self.fourier(log_sigma) # B*2ch\n",
        "        temb = self.temb_dense0(temb) # B*4ch\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb) # B*4ch\n",
        "\n",
        "        # 2) Downsampling\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "\n",
        "        # 3) Middle\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "\n",
        "        # 4) Upsampling\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "\n",
        "        # 5) Output\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "class VESDE:\n",
        "    \"\"\"\n",
        "    Variance Exploding SDE.\n",
        "\n",
        "    Parameters:\n",
        "        sigma_min (float) : Minimum sigma\n",
        "        sigma_max (float) : Maximum sigma\n",
        "        N (int) : Number of discretization steps\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma_min=0.01, sigma_max=50.0, N=1000):\n",
        "        self.smin, self.smax, self.N = sigma_min, sigma_max, N\n",
        "        self.T = 1.0\n",
        "        self.log_smin = math.log(self.smin)\n",
        "        self.log_smax = math.log(self.smax)\n",
        "\n",
        "    def sigma(self, t):\n",
        "        # t in [0,1]\n",
        "        return torch.exp(self.log_smin + t * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def marginal_prob(self, x0, t):\n",
        "        # mean = x0, std = sigma(t)\n",
        "        std = self.sigma(t).view(-1,1,1,1)\n",
        "        return x0, std\n",
        "\n",
        "    def diffusion_coeff(self, t):\n",
        "        # g(t) in VESDE\n",
        "        return self.sigma(t) * math.sqrt(2 * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def sample_prior(self, shape, device):\n",
        "        # Return a sample from the prior distribution.\n",
        "        return torch.randn(shape, device=device) * self.smax"
      ],
      "metadata": {
        "id": "tef7cC6Zq7sc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "KhyhsOUDvOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DDPMModel(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    ch=32,\n",
        "    ch_mult=(1, 2, 4),\n",
        "    num_res_blocks=2,\n",
        "    attn_resolutions={16},\n",
        "    dropout=0.1,\n",
        "    resamp_with_conv=True,\n",
        "    init_resolution=32\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "sde = VESDE(sigma_min=0.01, sigma_max=50.0, N=1000)\n",
        "\n",
        "model.train()\n",
        "for i in tqdm(range(20)):\n",
        "    loss_list = []\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        B = x.size(0)\n",
        "        t = torch.rand(B, device=device)\n",
        "        mean,std = sde.marginal_prob(x,t)\n",
        "        z = torch.randn_like(x)\n",
        "        xt = mean + std * z\n",
        "\n",
        "        score = model(xt, std.view(B))\n",
        "\n",
        "        loss = ((score*std + z).pow(2)).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss = np.mean(loss_list).item()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f'Epoch: {i}, Loss: {avg_loss:.4f} lr: {current_lr:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430,
          "referenced_widgets": [
            "52c159ee9a20414e93bb77fe516e68ab",
            "67b9b38dec4643c5b375a929dd116292",
            "199a529404d24b0fbef323c15b385165",
            "f22cea3a0f674e228d71c9c24cb45eac",
            "43e2ed86c2d64d35b2bb7ad76de1bb23",
            "d11ea6ba52a248ff87afd6224d83d9de",
            "8ba7fb794de54f5f823c713729f0df26",
            "b28e78519a964060821f3be3056dc8a9",
            "e9ae6a8d191849989a2d436f73f121d4",
            "b9162cfce65940859e04629f7044d191",
            "d4c294ad90d14288b3e725b40d5aaadd"
          ]
        },
        "id": "HWjC-b8XeVpq",
        "outputId": "e46464dd-4310-41e9-cb2e-7987d774f2b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 40.3MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52c159ee9a20414e93bb77fe516e68ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.6593 lr: 0.0020\n",
            "Epoch: 1, Loss: 0.3837 lr: 0.0020\n",
            "Epoch: 2, Loss: 0.3622 lr: 0.0020\n",
            "Epoch: 3, Loss: 0.2926 lr: 0.0020\n",
            "Epoch: 4, Loss: 0.2718 lr: 0.0020\n",
            "Epoch: 5, Loss: 0.3664 lr: 0.0020\n",
            "Epoch: 6, Loss: 0.2243 lr: 0.0020\n",
            "Epoch: 7, Loss: 0.2058 lr: 0.0020\n",
            "Epoch: 8, Loss: 0.1957 lr: 0.0020\n",
            "Epoch: 9, Loss: 0.1805 lr: 0.0002\n",
            "Epoch: 10, Loss: 0.1688 lr: 0.0002\n",
            "Epoch: 11, Loss: 0.1667 lr: 0.0002\n",
            "Epoch: 12, Loss: 0.1633 lr: 0.0002\n",
            "Epoch: 13, Loss: 0.1611 lr: 0.0002\n",
            "Epoch: 14, Loss: 0.1640 lr: 0.0002\n",
            "Epoch: 15, Loss: 0.1605 lr: 0.0002\n",
            "Epoch: 16, Loss: 0.1582 lr: 0.0002\n",
            "Epoch: 17, Loss: 0.1550 lr: 0.0002\n",
            "Epoch: 18, Loss: 0.1555 lr: 0.0002\n",
            "Epoch: 19, Loss: 0.1563 lr: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "lAVYVSHgvPuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def pc_sampler(model, sde, shape, device, snr=0.16, n_steps_pc=1):\n",
        "    \"\"\"\n",
        "    Predictor–Corrector sampler compatible with our DDPMModel and VESDE.\n",
        "    Refer to Algorithm 1,2,4 at Appendix F,G.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module) : Trained DDPMModel\n",
        "        sde : SDE object (e.g. VESDE) with methods\n",
        "        shape (tuple) : Shape of sampled image\n",
        "        device (torch.device) : Device\n",
        "        snr (float) : Signal to noise ratio\n",
        "        n_steps_pc (int) : Number of predictor-corrector steps\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Sampled image\n",
        "\n",
        "    Example:\n",
        "        >>> model = DDPMModel(ch=64,attn_resolution={16},init_resolution=32)\n",
        "        >>> sde = VESDE(sigma_min=0.01,sigma_max=50.0,N=1000)\n",
        "        >>> sample = pc_sampler(model,sde,shape=(64,3,32,32),device=device)\n",
        "        >>> print(sample.shape) # torch.Size([64,3,32,32])\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = sde.sample_prior(shape, device)\n",
        "    t_steps = torch.linspace(sde.T, 1e-3, sde.N, device=device)\n",
        "    dt = -(sde.T - 1e-3) / (sde.N - 1) # Each reverse step is adjusted to (0,1)\n",
        "\n",
        "    for t in tqdm(t_steps):\n",
        "        batch_t = t.repeat(shape[0])\n",
        "        sigma = sde.sigma(batch_t)\n",
        "\n",
        "        # Corrector (Langevin Dynamics)\n",
        "        for _ in range(n_steps_pc):\n",
        "            z = torch.randn_like(x)\n",
        "            score = model(x, sigma)  # gradient log p_t(x)\n",
        "            noise_norm = torch.norm(z.view(shape[0], -1), dim=1)\n",
        "            score_norm = torch.norm(score.view(shape[0], -1), dim=1)\n",
        "            step_size = (snr * noise_norm / (score_norm + 1e-12))**2\n",
        "            step_size = step_size.view(-1,1,1,1)\n",
        "            # Langevin update\n",
        "            x = x + step_size * score + torch.sqrt(2 * step_size) * z\n",
        "\n",
        "        # Predictor (Euler–Maruyama for the reverse SDE)\n",
        "        z = torch.randn_like(x)\n",
        "        g = sde.diffusion_coeff(t)\n",
        "        score = model(x, sigma)\n",
        "        # reverse SDE drift = -g^2 * score\n",
        "        x = x + (-g**2 * score) * dt + g * math.sqrt(-dt) * z\n",
        "\n",
        "    return x.clamp(-1,1)"
      ],
      "metadata": {
        "id": "LoD7432vh-I4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "q7aGTwqsvRjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(x0, idx=0):\n",
        "    \"\"\"\n",
        "    Visualize tensor data which is in the range of -1 to 1.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Tensor data\n",
        "        idx (int) : Index of tensor data ('idx'th image in the batch)\n",
        "    \"\"\"\n",
        "    img = x0[idx]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = (img + 1.0) / 2.0\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Sampled x_0\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qZi9-XqukRQ9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create samples"
      ],
      "metadata": {
        "id": "AxX_8HmL3t91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pc_sampler(model, sde, shape=(64,3,32,32), device=device, n_steps_pc=3)\n",
        "visualize_sample(sample.cpu().detach().numpy(), idx=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "afd87cc154254095ac74f2600c6d4452",
            "1d953149a79d4da8a0da7be09843e46d",
            "ecaf86ccfee0440bb0d359873d4ce8e5",
            "d99860f8f8c043898ad89b258d3f6e2c",
            "2f0854f154d9427b8e51cf200ba6606f",
            "c72b93b4b55643b3b04d6b7ed3881943",
            "f410d34b65ec45daa2952235c437fa83",
            "f607ba56f7314253b4041d76594a52c0",
            "3321d41ae38e4be08737e2dd849abe17",
            "b675d08eab954684b16c43714477acba",
            "a213caed300f4947bb343893c4380cb8"
          ]
        },
        "id": "SpjOufDYj7XD",
        "outputId": "40b27551-22c1-4d0a-924f-db59b90deaca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afd87cc154254095ac74f2600c6d4452"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFeCAYAAADnm4a1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIWBJREFUeJzt3W2MXXXZ7/HfWms/z0xn2tLCtJTWWqC0occT6CF4XhBB2oQC0VgTXqgtCJhQwOALEh9LeEhAEwuhCaFGuBN5ozyYYHgymBK9LSYCgpCTG70p4M1T25nO896z1957/c8L4xzG9s91ceS2gN9PQgLTq/+9Zu21f7Nor2v9kxBCEADgCOmxPgAA+LAiIAEggoAEgAgCEgAiCEgAiCAgASCCgASACAISACIISACIICBxTCVJohtuuOEDW++pp55SkiR66qmnPrA18a+LgPwYePHFF7V161atXLlStVpNy5cv1/nnn68777zzWB/av4wf//jHOu2001Sr1XTyySdz7j8mCMiPuH379unMM8/UCy+8oCuuuEK7d+/W5ZdfrjRNdccddxzrw/uXcPfdd+vyyy/X+vXrdeedd+rss8/Wtddeq9tuu+1YHxr+QaVjfQD4x9xyyy0aHBzU73//ew0NDc37tYMHDx6bg/oX0mq19O1vf1tbtmzRAw88IEm64oorVBSFbrrpJl155ZVauHDhMT5K/P/iDvIj7pVXXtH69euPCEdJWrp06bz/vvfee3Xuuedq6dKlqlarWrdune66664jft+qVat04YUX6qmnntKZZ56per2u008/fe7P9R566CGdfvrpqtVqOuOMM/SHP/xh3u/fvn27+vv7tX//fm3evFl9fX1atmyZbrzxRnkeHvXmm2/qsssu0/HHH69qtar169frnnvuOaLujTfe0Oc+9zn19fVp6dKluu6669Rut831W62W1q5dq7Vr16rVas19/fDhwxoeHtanP/1p9Xo9cx1J2rt3r0ZHR3XVVVfN+/qOHTs0MzOjRx55xLUOPqQCPtI2bdoUBgYGwosvvmjWbty4MWzfvj3s2rUr3HnnnWHTpk1BUti9e/e8upUrV4ZTTz01DA8PhxtuuCHs2rUrLF++PPT394f77rsvnHTSSeHWW28Nt956axgcHAxr1qwJvV5v7vdv27Yt1Gq1cPLJJ4cvf/nLYffu3eHCCy8MksJ3v/vdea8lKezcuXPuv995551w4oknhhUrVoQbb7wx3HXXXeHiiy8OksKuXbvm6prNZjjllFNCrVYL119/fbj99tvDGWecETZs2BAkhb17977nufjd734XsiwL11133dzXLrnkklCv18PLL79snsu/ufnmm4OkcODAgXlfb7fbIU3T8I1vfMO9Fj58CMiPuF/+8pchy7KQZVk4++yzw/XXXx+eeOKJkOf5EbXNZvOIr23evDmsXr163tdWrlwZJIV9+/bNfe2JJ54IkkK9Xg+vv/763NfvvvvuIwJp27ZtQVK45ppr5r5WFEXYsmVLqFQq4dChQ3Nf//uA/OpXvxqGh4fDyMjIvGO65JJLwuDg4Nz3cPvttwdJ4Wc/+9lczczMTFizZo0rIEMI4Zvf/GZI0zT8+te/Dvfff3+QFG6//Xbz973bjh07QpZlR/21JUuWhEsuueR9rYcPF/4X+yPu/PPP19NPP62LL75YL7zwgr7//e9r8+bNWr58uR5++OF5tfV6fe7fJyYmNDIyonPOOUf79+/XxMTEvNp169bp7LPPnvvvs846S5J07rnn6qSTTjri6/v37z/i2K6++uq5f0+SRFdffbXyPNeTTz551O8lhKAHH3xQF110kUIIGhkZmftn8+bNmpiY0HPPPSdJevTRRzU8PKytW7fO/f5Go6Err7zyvU/Yu9xwww1av369tm3bpquuukrnnHOOrr32Wvfvl/76v+uVSuWov1ar1eb9Lzw+evhLmo+BjRs36qGHHlKe53rhhRf085//XLt27dLWrVv1/PPPa926dZKk3/72t9q5c6eefvppNZvNeWtMTExocHBw7r/fHYKS5n5txYoVR/362NjYvK+naarVq1fP+9opp5wiSXrttdeO+n0cOnRI4+Pj2rNnj/bs2XPUmr/9xdPrr7+uNWvWKEmSeb9+6qmnHvX3HU2lUtE999yjjRs3qlar6d577z1iPUu9Xlee50f9tdnZ2Xk/lPDRQ0B+jFQqFW3cuFEbN27UKaecoksvvVT333+/du7cqVdeeUXnnXee1q5dqx/+8IdasWKFKpWKHn30Ue3atUtFUcxbK8uyo75G7OvhA9i542/H8KUvfUnbtm07as2GDRv+4dd5tyeeeELSX8Psz3/+sz7xiU+8r98/PDysXq+ngwcPzvtLsTzPNTo6qmXLln2gx4t/LgLyY+rMM8+UJL399tuSpF/84hdqt9t6+OGH590d7t2797/l9Yui0P79++fuGiXpT3/6k6S//i350SxZskQDAwPq9Xr67Gc/+57rr1y5Ui+99JJCCPPu+l5++WX3Mf7xj3/UjTfeqEsvvVTPP/+8Lr/8cr344ovz7qQtn/rUpyRJzzzzjC644IK5rz/zzDMqimLu1/HRxJ9BfsTt3bv3qHdvjz76qKT/97+cf7vze3ftxMSE7r333v+2Y9u9e/fcv4cQtHv3bpXLZZ133nlHrc+yTF/4whf04IMP6qWXXjri1w8dOjT37xdccIHeeuutud5DSWo2m9H/Nf97nU5H27dv17Jly3THHXfo3/7t33TgwAFdd9113m9P0l//THbRokVHtEvdddddajQa2rJly/taDx8u3EF+xF1zzTVqNpv6/Oc/r7Vr1yrPc+3bt08//elPtWrVKl166aWSpE2bNqlSqeiiiy7S1772NU1PT+tHP/qRli5dOneX+UGq1Wp6/PHHtW3bNp111ll67LHH9Mgjj+hb3/qWlixZEv19t956q/bu3auzzjpLV1xxhdatW6fDhw/rueee05NPPqnDhw9L0tzU0Fe+8hU9++yzGh4e1k9+8hM1Gg3X8d188816/vnn9atf/UoDAwPasGGDvve97+k73/mOtm7dOu9u8L3U63XddNNN2rFjh774xS9q8+bN+s1vfqP77rtPt9xyixYtWuRaBx9Sx/Bv0PEBeOyxx8Jll10W1q5dG/r7+0OlUglr1qwJ11xzzRG9eQ8//HDYsGFDqNVqYdWqVeG2224L99xzT5AUXn311bm6lStXhi1bthzxWpLCjh075n3t1VdfDZLCD37wg7mvbdu2LfT19YVXXnklbNq0KTQajXD88ceHnTt3zuuX/Nua727zCSGEAwcOhB07doQVK1aEcrkcTjjhhHDeeeeFPXv2zKt7/fXXw8UXXxwajUY47rjjwte//vXw+OOPm20+zz77bCiVSvPakEIIodvtho0bN4Zly5aFsbGx6O8/mj179oRTTz01VCqV8MlPfjLs2rUrFEXxvtbAh08SAvti44O1fft2PfDAA5qenj7WhwL8Q/gzSACI4M8ggaPI83zuzztjBgcH6XP8mCMggaPYt2+fPvOZz7xnzb333qvt27f/cw4IxwR/BgkcxdjYmJ599tn3rFm/fr2Gh4f/SUeEY4GABIAI/pIGACIISACIcP8lzb+/84qrrjrQZ9bk3aprrTTYT3UuUt/TV/LC97OgNFA2a3qRp7f8vXKw15KkkNt/ylHMdF1r1fvsSZJWMutay6tRst+DUs93/G3nXwonzaM/Ymyequ/8zzhOR9E9+kM6/l65O+mqy2VfQ0XwfTz7Ul9dt21/o95JpHawn9zezX1PZZ+xl5IkVTL7Optp+l7zc6ctd9VxBwkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABHuSZqZ1DficHjUfop03h0zayRpoL7ALopsQ3pEWdk3VZE27YmPbq8wayRJPd/ETeoom636JgTSXtOsKdq+41LFd86SzDF94dxvuu48tU3H99n0DbUopI77hHbHtdaU8ynqvWB/o42qb+KsqPnqmo73oJVPuNZKc/tzkqS+N7NcckxFSeobtK+ztOZby4s7SACIICABIIKABIAIAhIAIghIAIggIAEggoAEgAgCEgAi3I3i+WTLV+joLQ6eIkmFY5uEvG03DEtS5ty7sePox+4mvkZr7yPz61W7ATbr+RrFW57m9LZvrXbHtzXDbNnuyJ7t+K6fouxryK6l9tYepcLXQJ3IHjZIyr6BhLTuu+eo9uwLsus8ZyXft6k02MeWzPqOv+PY/qDR8X3oisy3Hcf4lP1ZT51bY3hxBwkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABHuSZpy8E2/BLXNmrTqy+VQzJg1tbpzW4BZ3+PfW027W79S9h1/7jgXkpTmju8h8U1V5G3HJIFzK4he8G2zkciezAkd3/vU8w1VqFOxz2018U1VjLfsbRLKie/489S+ZiWp5BjZyuXbpqLXdO5TMWBPH3Xbvn0qao6pnBn5jqvS8MVQq2NfHN6JM2m5q4o7SACIICABIIKABIAIAhIAIghIAIggIAEggoAEgAgCEgAi3I3i3dTX9Fwt2ZlbdmylIEmdxH5ke7nje0T/bObbPqDccHQqO7dSmJn0vWY32K9Z6+93rZV37ObcgczXAF7OfY3WRc9+P+s1554XwfmauX1uWyVf03DSsY+/WXU26k/76vqKmlnT3283dktSt+IcvHB8hCs1e/sPSSq37eb6nnM4oH3Y9xnuZnZdp+fLKS/uIAEggoAEgAgCEgAiCEgAiCAgASCCgASACAISACIISACIICABIMI9SVMp253/klQ4pmS6qa/DvtudMGt6ua8Lf6rpm3CohYpdVPLtC9BfrrrqmjP295B2fVMh3czeTiEPvuMKZd9US3AcWnPMd84qdd/ETVKyL916ZYFrrU5v1KwpOyd8krpv4qlSHjBrQuq7f6l0fRNbtczewiHkvm0eMtnfZzXzvZezJV9d5rjO2l3n9hNO3EECQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEEJAAEEFAAkCEe5KmPWVPaEhSmtr7neQ9ez8LSarInjCZbE251qr7BiFUr9inJC+arrWSrm96Z5FjH5C85dx7p2Of2wnP5iSSnNuTKCvs6aOQ+CYc2olvYqs7bb/vqXPvoLqjrln4zv+0c2KrcGw3M9jnewOStm/6ZaZtv++h8L1PXc/AWdVRJKnX9H04e449aZT4pqe8uIMEgAgCEgAiCEgAiCAgASCCgASACAISACIISACIICABIMLdKN5pT7rqBqr2dgpZ6mtUns3tR8kP9fkaQ/sz38+Cdtt+zczZtD1T+BpgS8F+zXLwPZZ+sGI359ZrvgbeZs93ebRzuzm62nBeaiVfo3K9ttCsKc/6mrYl+3wkHd97Wa87OsAllQv7PW+3fWulie/aznN7z4Lc8V5KUp9nO4VZ3+ekXbGHSyRptmUff9bzbe3hxR0kAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEuCdpal1fh/poMW6/aOp7RHyfYyqknPnW6vV80zsd2RMTs13fVEulbE9LSNL4lD090nBODA3U7OmLbvC9l+3c9/Mz9FfNmnLJd6kVzumjorC3AOmlvuNPHGt1ncfVn/m+z17h2GYjn3GtlfV832enal+3aZ9vyqrbso+tE5zHVTi/z7Z93XZT3/SOF3eQABBBQAJABAEJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQ4Z6kqaa+0v66vSdNkvn2HUlTe3qh49jbQ5KaXd9reuZtBvr7XWvVnD9+spJjsiX17dsxOW1PJQw0FrvWqtbsCRNJCsGeZso7vumdIN8kRLew1+vIvhYlqenYO6Vc9Z3/mY7z2u7ar5n4llLFWVg4PiulvOlaazK3PylFyZ4WkqTQ9WVL6vkM9/smnry4gwSACAISACIISACIICABIIKABIAIAhIAIghIAIggIAEgwt0oPjnrayAtVXv2WuOHXGsdf+JpZs1U27f9QXNiwlVXczRtz2rItVZLvubozozdkF3p853/iuOR/1Nj77jWylPfua2VHFtjJL5LLclqrrpOx77Omo6tFCRJjv7jvOfbFmDS95arWrFftOIczuh573McwwZ57hu86Lbt97w76zuu6V7LVVcr203gvVnfWl7cQQJABAEJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQQUACQIR7kmam8E1VlFr29EJSrbrWOnDwoFkz2/Ot1Wj0ueqKpuP4a77TNtuZdtWljsfETxW+x+rPOt6nouObMKnnvtdcOGSf21ZhT75IUqnkvM4cP9rztu/n/0DdnjBxXv4q+waBVK7Y122v65tq6c36xnfSqn0+Zjv2hIwkZWHKrCnJOeXmHD+q1ezPSbnP9zn34g4SACIISACIICABIIKABIAIAhIAIghIAIggIAEggoAEgAh3o/hU8DWQVtp2Q3C16lwrsxt4B3194sqnfI/Mn+2WzZpu07dWOXX+/HFsH9Dn67NW0uiYNY2676SlgwOuuvGe3RBccZ6K3NmRnST2tRFS3/s03bO3s+h0E9daCr66orCbo4Nzxwglvkbrdseu628436ihhWbJ5EHfNiH9VWeje6XfrCkXbLkAAP8UBCQARBCQABBBQAJABAEJABEEJABEEJAAEEFAAkAEAQkAEe5JmtQ5SZN37MfEp86XzSr2VEIv+B5LnzumPSQpUdus6bTtR79LUsh8rzlUtidbqiV7wkeSsvakWZPXfPsCFE3fhEPFsc9A07ktwIJ+37WRVO1zm7V9Uy3dwp74CI4pFEnqJs5H/meOibPU9563nJ+BStPeQiPvc57/ln0+OhVfZpTLvpGhTteekumMjLrW8uIOEgAiCEgAiCAgASCCgASACAISACIISACIICABIIKABIAIAhIAItyTNJPOSYjBqr1vRC3zTXK0OvaEQ1r4pg1CsKcIJKmb25M0/fJNCCSZb5JjZsaefunU7D1YJKkzZU8bVFrOzU5Kvr1rEsdbkHR8rzniOBeS1O+YWMm7vgmTcs9zbfump4Yy30eqGuzrbMZxLUpSOfNd281kwqwpOgtca2WpPQk07Zi2kaSac2Opfsc035Scm1Q5cQcJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQQUACQIS7UbzcfNZV166sMWsqveNca+WF3Wjd6PmasUPwNfoODCy213I2uk91Z1x1s23751StbTf5SlK9bjfqy9k0n5Z856wo7OPP6r5GdznOhSS1x+yG+KqcWy5U7Y9Bpec7ruBsTm86Dm1cvusnKXzvU1axr40guwFcknrBbgKv9fmusyz3nduxjn0+qs7Pphd3kAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEEJAAEOGepPn1b3/sKxxYbpaUtcS11PCK9XbNopWutdYs9dWVM/tnRlL3TRtMHvJNVRy3ZNCsCVPBtdbAoL0VQdr1PQo/SRuuunKwz9ms71Sok9jbbEhSt+iYNaHim+QIPXvPiFbbt/1Bz7V9gyTHxErW8G0n4quSmqn9ce9O+7bGKAX7/JdLviMrct+13Zywrw3nUJEbd5AAEEFAAkAEAQkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABDhnqSZ6Np7gEjS7Jv/YdbUOn9xrTXdPmDWvFby7TvyH9kyV13/0LBZs/KkFa61FjcGXHUDyfFmTbnfOZXQ/C+zZqLjmzCpZL5JmlapataE1K6RJDV8U0qZ423Pc99US5Ha4xe1uu9cePbnkaSJd94waxrOfXyyuj09JUklx5RPVvYdf7dqn4+yc3zK945LvWBnULvwTjL5cAcJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQQUACQIS7UXzBGl9d5z+nzZpe066RpLfGR8wab5PsSPA1rdZn9ps1L7/ma7Q+qeo7tsUr7UbxQ/m4a63pt94ya0Y7zufSt2uusrRmX0YDif2IfklaUPId2+BCewuNdav/p2utdNAeDmjVl7rWqld9Df2hZg8RTDanXGslLd/HOHO8neXUNxBSzNhbM6TyXT+9nm+bh0Njdh60xyZda3lxBwkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABHuSZoVJ/gmIQaqdt3oaL9rrV7XfuR8fajiWqs/8X2rpcR+ln/5sO81w5/HXHXNzJ4kGJkdd63V6TTNmq582x/UMt/D8Afqdt1A1zcVMhDGXXXZ1IRZM33It2XBYG2xXRTsKQ5J6h4Orrqli+yJm7w65FrrnaZv+qiS2NNkedf3nhc9e+ImJL77r5D7JmnUsyfYmlP29f9+cAcJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQQUACQIS7Ubzc8zU9Lx60m8D7s3Wutd6esNdKqr5HrM/M+JqjhwftR+sPOZp8JWnoBF+jcm/WbnouL1ntWquYmTFrcsdWFpI0lfouD89y9QW+BupS2d6KQJJqM3bT8/Ft+7xK0rLpl82axoKFrrWOW9B11Y1P2k3P7eN8n5MDue967JXthuxuy9e03Szb91bNTtu1VvCV6YQFy8yaJZlzOxEn7iABIIKABIAIAhIAIghIAIggIAEggoAEgAgCEgAiCEgAiCAgASDCPUnT7Pm69T9RsScO+gb/t2ut01b/L7PmnTd/51rr4IG3XXU1x1RCo+17LH2Y8G1T8c4Be0pp4bi9FYQkpcF+FH5/z7HFgKQks6dyJOnAxCGzptWypyAkacEKe5JJknoTr5o1h/YfcK1VGbHPf6fqm9A4XPfVDS08zqxp1U91rbUg9LnqWjP2dVstfFsWtLo1syat+abXKr6BMxXB/jwtWOK7fry4gwSACAISACIISACIICABIIKABIAIAhIAIghIAIggIAEggoAEgAj3JE0nWeCqW1G1J2mOk73XjCT918gbZk3ZsW+NJJ068ElXXWX0L47XnHKt1Svb+45IUtZvTxw0CntCRpL6UnuS47BjPxFJSvp8+7CsbtjvwaRz35Sxtm9Pmmq6yKwZmbX3rZGk/kF7qqWT+d7LyT7flFLeWGHWzMqeVpGkPPj2Zer17PMRct+eOrWSvVYj8X02m/m0qy6r2JM0RfrB3vNxBwkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJAhLtRvOZs4H0nsZecOPh/XGsdHM3Nmr6BVa61ylXflgWlpt10W+35HuU/U7K3IpCkBUN2c+7i1Ne0nbTt8/9Wr+1a663Ut+VCmthN4KO5r2m722q46qpl+/vMSr5zNtuy1wp5xbVWf933yP9Sxa4bSO3rX5J6zib2St2+H8pS3/lPOnZzd3fmoGutEHzntlXY5yN8wLd83EECQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEEJAAEEFAAkCEe5Lmjekx34ITfXbRjL0tgCS9PWZvbZAlr7jWqs/4uvWb+YhZ01cbd61VZL7pkSWOSY5W8G1ZkDTt18yrQ661agt852ymYm8ZMdTqudYaGBpy1S3MV5s1vbpv+qs9dtguajVdayXyXdtToxP2Wj3fx7Nc8U3SJA17Sqbq2NZAkrKSfW/V7fjOWSn1XRsTk/Z6fQsGXWt5cQcJABEEJABEEJAAEEFAAkAEAQkAEQQkAEQQkAAQQUACQAQBCQAR7kmaw//p25+kvnDIftGO72V7yz5h1kwO+aZVpp37drzt2FOnU/X9XKknjqkiSaMzdvf/6sHTXGt1J+39bbq5PfkiSUtXrXLVJTV7+qL62rhrrXLjeFdd6D/RrBmq+t7zgYElZk133Lm/kGN/HklKk2DWlFLfVE5/4XvN8cJeb6JoudbKZu2plqJkf4+SlGW+63HhQN2saVTtmveDO0gAiCAgASCCgASACAISACIISACIICABIIKABIAIAhIAItyN4ksy+xH3ktSXrTNr0spC11qlxTW7KHM8Ll/ScX2Jq25pYde9U/E1wKZJ21W3KFll1tQX2U3zklRyNIqHjuO8ShrJfcdfnrbPxwkLfA3gHefWEp2S3QQ+PmGfC0mqL7Rfc3bxItdavbZvcKFctxuaBzq+azZd6Gsob0/Z2zwUha+5flr2tbF0wWLXWhXnbVoya3+fIfU1untxBwkAEQQkAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABHuSZo+Z5TmXbsTv+x4dLoktaftbv1addq1Vq/lm4Q4YegEs6YRfNtPNCu+SY5qz3E+mr61SvWKWVNv+LaCaMzaa0lSL8nt18x90zuzU1OuuumRP5s1xYDvov3LqF1TVsO1Vs+5tcdgbm9TMRZGXGtVp3uuuoHF9tYGpcT32WyO2NfjjONalKSuYypKkorcvs66Xd/0lxd3kAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEuBvF8+YC34L1frNmfMLX2Lowsx+xfuLAoGut3LnNQFa1H3Pf17ObySWpscC3NUNn0v4+px1N85JUr9rfZwh2k7Ikldq+4+/O2I2+YyVfA/hM4ujallQK9msWGnat1Q72sZWd22c0Fi5x1RWOczs6etC1Vv+sd3DBvjaqxXGutfJ20369Cd9xTea+GOq0Hecj+2Dv+biDBIAIAhIAIghIAIggIAEggoAEgAgCEgAiCEgAiCAgASCCgASACPckzZqV/8NVdygbMmvKHd/j67ujb5o1vWHfWtVh3zYD+YT9WPdybcC1VvOAbxKlP9jrZQ3fo/ArlbJZM9Nqudbq9XwTT+q3H+U/25l1LZU17EksSVrQZ08f9Rq+R/6PjtvnrN7nu87k3D4gTxxbk/T5zkXW9d3nZDP2ezCVjLvWqpftaay063vPZ4P9mZOkWmbXlVLfZ86LO0gAiCAgASCCgASACAISACIISACIICABIIKABIAIAhIAIghIAIhwT9LMFEtddX1Ddvf/spJvQqNTcnTOJ0Outbple9pDksaKw2ZNfcq3P0mp53tNVe0pn9DxTWhkjgmTwYpvf6F8wPc+jRw4YNZ0Z5176riqpOlZe0+UfHbctVYz2JM0We6b0EjGfFNK7WC/T3lzwrVWK/ed28UnHG/WpFO+/YpGpu39ZroLfRMyi5b49pUaPezYOyifdq3lxR0kAEQQkAAQQUACQAQBCQARBCQARBCQABBBQAJABAEJABHuRvHOoG+bgTSxGzXrNWfT7aDdNtwt+5qZD7+531XXcDxaf/GCYdda5cmuqy5zbFkwOuprGh4bsx9zH5w7KSizG6glKS3shuCs5DsXU85G5akZu6G/03VuLdG2m/BbtcWutcZbk666NE3Mmplp33BAWvg+xm++/bZZ00h9ww2zmd3oPlP47r8WLPJthzLUbzeUt8ZoFAeAfwoCEgAiCEgAiCAgASCCgASACAISACIISACIICABIIKABICIJITgG2sBgH8x3EECQAQBCQARBCQARBCQABBBQAJABAEJABEEJABEEJAAEEFAAkDE/wW+OyWkw/XaqAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
