{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Code Implementation**\n",
        "\n",
        "This code implements PPO algorithm to finetune trained DDPM. <br>\n",
        "Refer to:<br>\n",
        "\n",
        "* Paper1 : Proximal policy optimization algorithm (2017)\n",
        "* Paper2 : 3D-HLDM: ... (2024)\n",
        "\n",
        "We have a DDPM model trained with CelebA dataset. Reward model is trained with real dataset of CelebA and fake dataset of synthetic image generated by DDIM. Using this model, we aim to finetune trained DDPM to attain higher reward model score while generating realistic image by sampling process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the formula of L_CLIP used in PPO algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min\\big(r_t(\\theta)\\hat{A}_t, \\operatorname{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\big) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We regard the entire reverse process as a single episode with a single timestep. $\\hat{A}_t$ in the formula is calculated by the trained reward model and its parameters are fixed during the train. \n",
        "$$r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
        "can be calculated analytically using the knowledge of DDPM as in the below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\log r(\\theta) = \\sum_{t=1}^T \\frac{\\Vert x_{t-1} - \\mu_{old}(x_t,t) \\Vert ^2 - \\Vert x_{t-1} - \\mu_\\theta(x_t,t) \\Vert ^2}{2\\tilde{\\beta}_t} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can compute the value of $L_{CLIP}$ in each episode. In each episode, we iterate the entire DDIM reverse process to obtain $r(\\theta)$ and $\\hat{A}$ and backpropagate it. But this method requires too large GPU memory since it preserves all the gradient graph of the whole timestep of the reverse process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid GPU memory issue, we divide the backpropagation of a single episode into two steps. In the first step, we run the reverse process **without** gradient calculating, and only save $x_t$, $\\epsilon_t$ in all timesteps and calculate $\\hat{A}$ with the resulting $x_0$. In the second step, we compute the gradient of $\\log r_t(\\theta)$ and compute the gradient of each timestep **one by one** so that we do not have to save the whole computation graph. Then we can compute the gradient of $L_{CLIP}$ by the below formula using the gradient of $\\log r_t(\\theta)$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\nabla_\\theta L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\hat{A} \\, r(\\theta) \\, \\sum_{t=1}^T \\nabla_\\theta \\log r_t(\\theta) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, copy, math, time, random\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDPM Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Model\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def get_timestep_embedding(t, channel):\n",
        "    half = channel // 2\n",
        "    device = t.device\n",
        "    inv_freq = torch.exp(-math.log(10000) * torch.arange(half) / half).to(device)\n",
        "    args = t.float().unsqueeze(1) * inv_freq.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "    return emb\n",
        "\n",
        "class GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, temb_channels=256, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.norm1 = GroupNorm(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "        h_temb = swish(temb)\n",
        "        h_temb = self.temb_proj(h_temb)\n",
        "        h_temb = h_temb[:, :, None, None]\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h)\n",
        "        x = self.conv_shortcut(x)\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h)\n",
        "        k = self.k(h)\n",
        "        v = self.v(h)\n",
        "        q = q.view(B, C, H * W).permute(0, 2, 1)\n",
        "        k = k.view(B, C, H * W).permute(0, 2, 1)\n",
        "        v = v.view(B, C, H * W).permute(0, 2, 1)\n",
        "        scale = q.shape[-1] ** -0.5\n",
        "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * scale, dim=-1)\n",
        "        h_ = torch.bmm(attn, v)\n",
        "        h_ = h_.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "        h_ = self.proj_out(h_)\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 4),\n",
        "        num_res_blocks=2,\n",
        "        attn_resolutions={32},\n",
        "        dropout=0.0,\n",
        "        resamp_with_conv=False,\n",
        "        init_resolution=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "        self.temb_ch = ch * 4\n",
        "        self.temb_dense0 = linear(self.ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "        self.norm_out = GroupNorm(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb_dense0(temb)\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb)\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "# Sampling\n",
        "def p_sample_ddim(model, x_t, t_cur, t_prev, alphas_cumprod, eta=0.0):\n",
        "    alpha_bar_t = alphas_cumprod[t_cur - 1]\n",
        "    if t_prev > 0:\n",
        "        alpha_bar_prev = alphas_cumprod[t_prev - 1]\n",
        "    else:\n",
        "        alpha_bar_prev = torch.tensor(1.0, device=x_t.device)\n",
        "    sigma_t = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * torch.sqrt(1 - alpha_bar_t / alpha_bar_prev)\n",
        "    B = x_t.size(0)\n",
        "    t_tensor = torch.full((B,), t_cur, device=x_t.device, dtype=torch.long)\n",
        "    eps_theta = model(x_t, t_tensor)\n",
        "    sqrt_ab_t = torch.sqrt(alpha_bar_t)\n",
        "    sqrt_ab_prev = torch.sqrt(alpha_bar_prev)\n",
        "    x0_pred = (x_t - torch.sqrt(1 - alpha_bar_t).view(-1, 1, 1, 1) * eps_theta) / sqrt_ab_t.view(-1, 1, 1, 1)\n",
        "    dir_xt = torch.sqrt(torch.clamp(1 - alpha_bar_prev - sigma_t**2, min=0.0)).view(-1, 1, 1, 1) * eps_theta\n",
        "    noise = torch.randn_like(x_t) if t_prev > 0 else torch.zeros_like(x_t)\n",
        "    x_prev = sqrt_ab_prev.view(-1, 1, 1, 1) * x0_pred + dir_xt + sigma_t.view(-1, 1, 1, 1) * noise\n",
        "    return x_prev, eps_theta.detach().cpu(), x_t.detach().cpu()\n",
        "\n",
        "def sample_ddim(model, shape, alphas_cumprod, device, ddim_steps, eta=0.0):\n",
        "    steps_log = {\"t\": [], \"eps_t\": [], \"x_t\": []}\n",
        "    num_timesteps = alphas_cumprod.shape[0]\n",
        "    x = torch.randn(shape, device=device)\n",
        "    idx_lin = torch.linspace(0, num_timesteps - 1, steps=ddim_steps + 1, device=device)\n",
        "    idx0 = idx_lin.round().long()\n",
        "    idx0 = torch.cat([torch.tensor([0, num_timesteps - 1], device=device, dtype=torch.long), idx0]).unique(sorted=True)\n",
        "    seq_asc = idx0 + 1\n",
        "    seq_rev = torch.flip(seq_asc, dims=[0])\n",
        "    seq = torch.cat([seq_rev, torch.tensor([0], device=device, dtype=torch.long)])\n",
        "    prev_t = seq[0].item()\n",
        "    for next_t in seq[1:]:\n",
        "        t_cur = prev_t\n",
        "        t_prev = next_t.item()\n",
        "        x, eps_t, x_t_snap = p_sample_ddim(\n",
        "            model,\n",
        "            x,\n",
        "            t_cur,\n",
        "            t_prev,\n",
        "            alphas_cumprod,\n",
        "            eta\n",
        "        )\n",
        "        steps_log[\"t\"].append(t_cur)\n",
        "        steps_log[\"eps_t\"].append(eps_t)\n",
        "        steps_log[\"x_t\"].append(x_t_snap)\n",
        "        prev_t = t_prev\n",
        "    return x, steps_log\n",
        "\n",
        "def get_beta_alpha_linear(beta_start=0.0001, beta_end=0.02, num_timesteps=1000):\n",
        "    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
        "    betas = torch.tensor(betas)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    return betas, alphas, alphas_cumprod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=3, stride=1, padding=1, pool=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.act = nn.GELU()\n",
        "        self.pool = nn.AvgPool2d(2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_channels=32, dropout=0.2):\n",
        "        super().__init__()\n",
        "        b = base_channels\n",
        "        self.blocks = nn.Sequential(\n",
        "            ConvBlock(in_channels, b, pool=True),\n",
        "            ConvBlock(b, b*2, pool=True),\n",
        "            ConvBlock(b*2, b*4, pool=True),\n",
        "            ConvBlock(b*4, b*8, pool=True),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(b*8, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        x = self.head(x)\n",
        "        return x.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_mu_from_eps(x_t, t_idx, eps_pred, alphas, alpha_bars):\n",
        "    \"\"\"\n",
        "    Compute mu(mean) from epsilon which is predicted by model.\n",
        "    The value of mu is used to compute the ratio of the value of policy.\n",
        "\n",
        "    Parameters:\n",
        "        x_t (torch.Tensor): Predicted image tensor at timestep t during the sampling process.\n",
        "        t_idx (int): Number of the timestep.\n",
        "        eps_pred (torch.Tensor): The value of epsilon predicted by model.\n",
        "        alphas (torch.Tensor): Tensor of alphas.\n",
        "        alpha_bars (torch.Tensor): Tensor of alpha_bars.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: mu(mean)\n",
        "    \"\"\"\n",
        "    T = alphas.shape[0]\n",
        "    if t_idx < 0:\n",
        "        t_idx = 0\n",
        "    if t_idx > T - 1:\n",
        "        t_idx = T - 1\n",
        "\n",
        "    alpha_t = alphas[t_idx].to(x_t.device, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "    beta_t  = (1.0 - alphas[t_idx].to(x_t.device, dtype=torch.float32)).view(-1, 1, 1, 1)\n",
        "    abar_t  = alpha_bars[t_idx].to(x_t.device, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "    denom = torch.sqrt(torch.clamp(1.0 - abar_t, min=1e-12))\n",
        "    num   = beta_t * eps_pred.to(torch.float32)\n",
        "    mu = (x_t.to(torch.float32) - (num / denom)) / torch.sqrt(torch.clamp(alpha_t, min=1e-12))\n",
        "    return mu.to(x_t.dtype)\n",
        "\n",
        "def compute_log_r_sum(\n",
        "    cur_model,\n",
        "    steps_log,\n",
        "    x0_cpu,\n",
        "    alphas_cumprod,\n",
        "    device,\n",
        "    eta,\n",
        "    eps_to_mu_fn,\n",
        "    per_step_clip=5.0,\n",
        "    global_clip=10.0,\n",
        "    sigma_floor=1e-3\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the log of the sum of the r.\n",
        "    r is the value of the ratio of the policy probability function, as in the PPO algorithm.\n",
        "    This function not only computes log_r_sum, but also the gradients of each parameters.\n",
        "\n",
        "    Parameters:\n",
        "        cur_model (torch.nn.Module): The model updated most recently.\n",
        "        steps_log (dict): Dictionary of t, x_t, epsilon of each timestep while sampling.\n",
        "        x0_cpu (torch.Tensor): Image tensor computed by reverse(sampling) process.\n",
        "        alphas_cumprod (torch.Tensor): Tensor of alpha_bars.\n",
        "        device (torch.device): Device.\n",
        "        eta (float): The ratio of probabilistic process in the DDPM(DDIM) reverse process.\n",
        "        eps_to_mu_fn (function): Function to compute mu(mean) from epsilon.\n",
        "        per_step_clip (float): Clipping value of log r_t in each timestep.\n",
        "        global_clip (float): Clipping value of log_r_sum.\n",
        "        sigma_floor (float): Minimum clipping value of sigma to avoid sigma to be too small.\n",
        "\n",
        "    Returns:\n",
        "        float: log_r_sum (summed over timesteps).\n",
        "        list[torch.Tensor]: sum_grads (same shape as model params).\n",
        "    \"\"\"\n",
        "    params = [p for p in cur_model.parameters() if p.requires_grad]\n",
        "    sum_grads = [torch.zeros_like(p) for p in params]\n",
        "    log_r_sum = 0.0\n",
        "\n",
        "    Tlist = steps_log[\"t\"]\n",
        "    Xlist = steps_log[\"x_t\"]\n",
        "    Elist = steps_log[\"eps_t\"]\n",
        "\n",
        "    total_steps = len(Tlist)\n",
        "\n",
        "    for k in range(total_steps):\n",
        "        t_cur  = int(Tlist[k])\n",
        "        t_prev = int(Tlist[k+1]) if (k+1) < total_steps else 0\n",
        "\n",
        "        if t_prev == 0:\n",
        "            continue\n",
        "\n",
        "        x_t = Xlist[k].to(device).detach()\n",
        "        x_tprev = Xlist[k+1].to(device).detach() if (k+1) < len(Xlist) else x0_cpu.to(device).detach()\n",
        "        eps_old = Elist[k]_
