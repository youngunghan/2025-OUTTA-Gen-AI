{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5TZthvihO856",
        "iAH_874YkLMT",
        "e_cbYfz_LUva",
        "ge1W6K1SkOaR",
        "TpYT7RQvYsTx",
        "lcok62SE7OnG"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Code Implementation**\n",
        "\n",
        "This code implements PPO algorithm to finetune trained DDPM. <br>\n",
        "Refer to:<br>\n",
        "\n",
        "\n",
        "*   Paper1 : Proximal policy optimization algorithm (2017)\n",
        "*   Paper2 : 3D-HLDM: ... (2024)\n",
        "\n",
        "We have a DDPM model trained with CelebA dataset. Reward model is trained with real dataset of CelebA and fake dataset of synthetic image generated by DDIM. Using this model, we aim to finetune trained DDPM to attain higher reward model score while generating realistic image by sampling process.\n"
      ],
      "metadata": {
        "id": "jROjQUIzDsjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the formula of L_CLIP used in PPO algorithm."
      ],
      "metadata": {
        "id": "gt_jqxNIHlaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min(r_t(\\theta)\\hat{A}_t, \\mathrm{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t) \\right]$"
      ],
      "metadata": {
        "id": "HkECsi82V-We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We regard the entire reverse process as a single epsiode with a single timestep. $\\hat{A}_t$ in the formula is caculated by the trained reward model and its paramters are fixed during the train. $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ can be caculated ananlytically using the knowledge of DDPM as in the below."
      ],
      "metadata": {
        "id": "hLIxkcv_HxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\log r(\\theta) = \\sum_{t=1}^T \\frac{\\Vert x_{t-1} - \\mu_{old}(x_t,t) \\Vert ^2 - \\Vert x_{t-1} - \\mu_\\theta(x_t,t) \\Vert ^2}{2\\tilde{\\beta}_t} $"
      ],
      "metadata": {
        "id": "lJmKbbeaWQwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can compute the value of L_CLIP in each episodes. In each episodes, we iter the entire DDIM reverse process to obtain $r(\\theta)$ and $\\hat{A}$ and backpropagate it. But this method requires too large GPU memory since it preserves all the gradient graph of the whole timestep of the reverse process."
      ],
      "metadata": {
        "id": "mIH9XEN0LLtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid GPU memory issue, we divide the backpropagation of a single episode into two steps. In the first step, we run the reverse process without gradient calculating, and only saves $x_t$, $\\epsilon_t$ in all timesteps and calculate $\\hat{A}$ with the resulting $x_0$. In the second step, we compute the gradient of $\\log r_t(\\theta)$ and compute the gradient of each timestep. Note that we compute the gradient step by step so that we do not have to save the whole computation graph. This method let us save the GPU memory. Then we can compute the gradient of L_CLIP by the below formula using the gradient of $\\log r_t(\\theta)$. The algorithm is implemented in the function L_CLIP_two_pass."
      ],
      "metadata": {
        "id": "H48ukOKFNBbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\nabla_\\theta L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\hat{A} r(\\theta) \\sum_{t=1}^T \\nabla_\\theta \\log r_t(\\theta) \\right]$"
      ],
      "metadata": {
        "id": "PCPfwP7LWow_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "5TZthvihO856"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41vpt7NAOr1J",
        "outputId": "2bacc316-f6d9-49d4-d467-749874be5017"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DzLhYWP6aCBt"
      },
      "outputs": [],
      "source": [
        "import os, copy, math, time, random\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import random\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDPM Setup"
      ],
      "metadata": {
        "id": "iAH_874YkLMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def get_timestep_embedding(t, channel):\n",
        "    half = channel // 2\n",
        "    device = t.device\n",
        "    inv_freq = torch.exp(-math.log(10000) * torch.arange(half) / half).to(device)\n",
        "    args = t.float().unsqueeze(1) * inv_freq.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "    return emb\n",
        "\n",
        "class GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, temb_channels=256, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.norm1 = GroupNorm(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "        h_temb = swish(temb)\n",
        "        h_temb = self.temb_proj(h_temb)\n",
        "        h_temb = h_temb[:, :, None, None]\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h)\n",
        "        x = self.conv_shortcut(x)\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h)\n",
        "        k = self.k(h)\n",
        "        v = self.v(h)\n",
        "        q = q.view(B, C, H * W).permute(0, 2, 1)\n",
        "        k = k.view(B, C, H * W).permute(0, 2, 1)\n",
        "        v = v.view(B, C, H * W).permute(0, 2, 1)\n",
        "        scale = q.shape[-1] ** -0.5\n",
        "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * scale, dim=-1)\n",
        "        h_ = torch.bmm(attn, v)\n",
        "        h_ = h_.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "        h_ = self.proj_out(h_)\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 4),\n",
        "        num_res_blocks=2,\n",
        "        attn_resolutions={32},\n",
        "        dropout=0.0,\n",
        "        resamp_with_conv=False,\n",
        "        init_resolution=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "        self.temb_ch = ch * 4\n",
        "        self.temb_dense0 = linear(self.ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "        self.norm_out = GroupNorm(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb_dense0(temb)\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb)\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "# Sampling\n",
        "def p_sample_ddim(model, x_t, t_cur, t_prev, alphas_cumprod, eta=0.0):\n",
        "    alpha_bar_t = alphas_cumprod[t_cur - 1]\n",
        "    if t_prev > 0:\n",
        "        alpha_bar_prev = alphas_cumprod[t_prev - 1]\n",
        "    else:\n",
        "        alpha_bar_prev = torch.tensor(1.0, device=x_t.device)\n",
        "    sigma_t = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * torch.sqrt(1 - alpha_bar_t / alpha_bar_prev)\n",
        "    B = x_t.size(0)\n",
        "    t_tensor = torch.full((B,), t_cur, device=x_t.device, dtype=torch.long)\n",
        "    eps_theta = model(x_t, t_tensor)\n",
        "    sqrt_ab_t = torch.sqrt(alpha_bar_t)\n",
        "    sqrt_ab_prev = torch.sqrt(alpha_bar_prev)\n",
        "    x0_pred = (x_t - torch.sqrt(1 - alpha_bar_t).view(-1, 1, 1, 1) * eps_theta) / sqrt_ab_t.view(-1, 1, 1, 1)\n",
        "    dir_xt = torch.sqrt(torch.clamp(1 - alpha_bar_prev - sigma_t**2, min=0.0)).view(-1, 1, 1, 1) * eps_theta\n",
        "    noise = torch.randn_like(x_t) if t_prev > 0 else torch.zeros_like(x_t)\n",
        "    x_prev = sqrt_ab_prev.view(-1, 1, 1, 1) * x0_pred + dir_xt + sigma_t.view(-1, 1, 1, 1) * noise\n",
        "    return x_prev, eps_theta.detach().cpu(), x_t.detach().cpu()\n",
        "\n",
        "def sample_ddim(model, shape, alphas_cumprod, device, ddim_steps, eta=0.0):\n",
        "    steps_log = {\"t\": [], \"eps_t\": [], \"x_t\": []}\n",
        "    num_timesteps = alphas_cumprod.shape[0]\n",
        "    x = torch.randn(shape, device=device)\n",
        "    idx_lin = torch.linspace(0, num_timesteps - 1, steps=ddim_steps + 1, device=device)\n",
        "    idx0 = idx_lin.round().long()\n",
        "    idx0 = torch.cat([torch.tensor([0, num_timesteps - 1], device=device, dtype=torch.long), idx0]).unique(sorted=True)\n",
        "    seq_asc = idx0 + 1\n",
        "    seq_rev = torch.flip(seq_asc, dims=[0])\n",
        "    seq = torch.cat([seq_rev, torch.tensor([0], device=device, dtype=torch.long)])\n",
        "    prev_t = seq[0].item()\n",
        "    for next_t in seq[1:]:\n",
        "        t_cur = prev_t\n",
        "        t_prev = next_t.item()\n",
        "        x, eps_t, x_t_snap = p_sample_ddim(\n",
        "            model,\n",
        "            x,\n",
        "            t_cur,\n",
        "            t_prev,\n",
        "            alphas_cumprod,\n",
        "            eta\n",
        "        )\n",
        "        steps_log[\"t\"].append(t_cur)\n",
        "        steps_log[\"eps_t\"].append(eps_t)\n",
        "        steps_log[\"x_t\"].append(x_t_snap)\n",
        "        prev_t = t_prev\n",
        "    return x, steps_log\n",
        "\n",
        "# Get alpha_bar\n",
        "def get_beta_alpha_linear(beta_start=0.0001, beta_end=0.02, num_timesteps=1000):\n",
        "    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
        "    betas = torch.tensor(betas)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    return betas, alphas, alphas_cumprod"
      ],
      "metadata": {
        "id": "PgaAUJSBkMRE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Model"
      ],
      "metadata": {
        "id": "e_cbYfz_LUva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=3, stride=1, padding=1, pool=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.act = nn.GELU()\n",
        "        self.pool = nn.AvgPool2d(2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_channels=32, dropout=0.2):\n",
        "        super().__init__()\n",
        "        b = base_channels\n",
        "        self.blocks = nn.Sequential(\n",
        "            ConvBlock(in_channels, b, pool=True),\n",
        "            ConvBlock(b, b*2, pool=True),\n",
        "            ConvBlock(b*2, b*4, pool=True),\n",
        "            ConvBlock(b*4, b*8, pool=True),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(b*8, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        x = self.head(x)\n",
        "        return x.view(-1)"
      ],
      "metadata": {
        "id": "smKScQ9rLWIX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO Algorithm"
      ],
      "metadata": {
        "id": "ge1W6K1SkOaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mu_from_eps(x_t, t_idx, eps_pred, alphas, alpha_bars):\n",
        "    \"\"\"\n",
        "    Compute mu(mean) from epsilon which is predicted by model.\n",
        "    The value of mu is used to compute the ratio of the value of policy\n",
        "\n",
        "    Parameters:\n",
        "        x_t (torch.Tensor) : Predicted image tensor at timestep t during the sampling process.\n",
        "        t_idx (int) : Number of the timestep.\n",
        "        eps_pred (torch.Tensor) : The value of epsilon predicted by model.\n",
        "        alphas (torch.Tensor) : Tensor of alphas.\n",
        "        alpha_bars (torch.Tensor) : Tensor of alpha_bars.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : mu(mean)\n",
        "    \"\"\"\n",
        "    T = alphas.shape[0]\n",
        "    if t_idx < 0:\n",
        "        t_idx = 0\n",
        "    if t_idx > T - 1:\n",
        "        t_idx = T - 1\n",
        "\n",
        "    alpha_t = alphas[t_idx].to(x_t.device, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "    beta_t  = (1.0 - alphas[t_idx].to(x_t.device, dtype=torch.float32)).view(-1, 1, 1, 1)\n",
        "    abar_t  = alpha_bars[t_idx].to(x_t.device, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "    denom = torch.sqrt(torch.clamp(1.0 - abar_t, min=1e-12))\n",
        "    num   = beta_t * eps_pred.to(torch.float32)\n",
        "\n",
        "    mu = (x_t.to(torch.float32) - (num / denom)) / torch.sqrt(torch.clamp(alpha_t, min=1e-12))\n",
        "\n",
        "    return mu.to(x_t.dtype)\n",
        "\n",
        "def compute_log_r_sum(cur_model, steps_log, x0_cpu,\n",
        "                      alphas_cumprod, device, eta, eps_to_mu_fn,\n",
        "                      per_step_clip=5.0, global_clip=10.0, sigma_floor=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the log of the sum of the r.\n",
        "    r is the value of the ratio of the policy probability function, as in the PPO algorithm.\n",
        "    This function not only computes log_r_sum, but also the gradients of each parameters.\n",
        "\n",
        "    Parameters:\n",
        "        cur_model (torch.nn.Module) : The model updated most recently.\n",
        "        steps_log (dictionary) : Dictionary of t, x_t, epsilon of each timestep while sampling.\n",
        "        x0_cpu (torch.Tensor) : Image tensor computed by reverse(sampling) process.\n",
        "        alphas_cumprod (torch.Tensor) : Tensor of alpha_bars.\n",
        "        device (torch.device) : Device.\n",
        "        eta (float) : The ratio of probabilistic process in the DDPM(DDIM) reverse process.\n",
        "        eps_to_mu_fn (function) : Function to compute mu(mean) from epsilon.\n",
        "        per_step_clip (float) : Clipping value of log r_t in each timestep.\n",
        "        global_clip (float) : Clipping value of log_r_sum.\n",
        "        simga_floor (float) : Minimum clipping value of sigma to avoid sigma to be too small.\n",
        "\n",
        "    Returns:\n",
        "        float : log_r_sum. Summation is done in each timestep.\n",
        "        list of tensors : sum_grads. List of gradients of each parameters. Summation is done in each timestep.\n",
        "    \"\"\"\n",
        "    params = [p for p in cur_model.parameters() if p.requires_grad]\n",
        "    sum_grads = [torch.zeros_like(p) for p in params]\n",
        "    log_r_sum = 0.0\n",
        "\n",
        "    Tlist = steps_log[\"t\"]\n",
        "    Xlist = steps_log[\"x_t\"]\n",
        "    Elist = steps_log[\"eps_t\"]\n",
        "\n",
        "    total_steps = len(Tlist)\n",
        "\n",
        "    # Iter each step and compute r in each step.\n",
        "    for k in range(total_steps):\n",
        "        t_cur  = int(Tlist[k])\n",
        "        t_prev = int(Tlist[k+1]) if (k+1) < total_steps else 0\n",
        "\n",
        "        # Skip deterministic last-step\n",
        "        if t_prev == 0:\n",
        "            continue\n",
        "\n",
        "        # Load data on GPU\n",
        "        x_t = Xlist[k].to(device).detach()\n",
        "        x_tprev = Xlist[k+1].to(device).detach() if (k+1)<len(Xlist) else x0_cpu.to(device).detach()\n",
        "        eps_old = Elist[k].to(device).detach()\n",
        "\n",
        "        # Compute mu_old. mu_old is computed by eps_old which is computed by old_model.\n",
        "        mu_old = eps_to_mu_fn(x_t, t_cur-1, eps_old).detach()\n",
        "\n",
        "        # Compute mu_cur. mu_cur is computed by eps_cur which is computed by cur_model.\n",
        "        t_tensor = torch.full((x_t.size(0),), t_cur, device=device, dtype=torch.long)\n",
        "        eps_cur  = cur_model(x_t, t_tensor)  # requires_grad=True\n",
        "        mu_cur = eps_to_mu_fn(x_t, t_cur-1, eps_cur)\n",
        "\n",
        "        # Compute sigma_t to use in DDIM sampling steps.\n",
        "        a_t = alphas_cumprod[t_cur-1].to(device).float()\n",
        "        if t_prev > 0:\n",
        "            a_prev = alphas_cumprod[t_prev-1].to(device).float()\n",
        "            num = torch.clamp(1.0-a_prev, min=0.0)\n",
        "            denom = torch.clamp(1.0-a_t, min=1e-12)\n",
        "            frac = torch.clamp(num/denom,  min=0.0, max=1.0)\n",
        "            term = torch.clamp(1.0 - (a_t/a_prev), min=0.0)\n",
        "            c1 = torch.sqrt(frac)\n",
        "            c2 = torch.sqrt(term)\n",
        "        else:\n",
        "            c1 = torch.tensor(1.0, device=device, dtype=torch.float32)\n",
        "            c2 = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
        "        sigma_t = (eta * c1 * c2).to(x_t.dtype)\n",
        "        sigma_t = torch.clamp(sigma_t, min=float(sigma_floor))\n",
        "\n",
        "        # Compute log r_t.\n",
        "        diff_cur = (x_tprev - mu_cur).pow(2).mean() # Use mean so that the scale of the value is preserved.\n",
        "        diff_old = (x_tprev - mu_old).pow(2).mean()\n",
        "        denom = 2.0 * (sigma_t**2) + 1e-12\n",
        "        log_r_t  = - (diff_cur-diff_old) / denom\n",
        "\n",
        "        # Clamp before autodiff to avoid exploding grads.\n",
        "        log_r_t_clamped = torch.clamp(log_r_t, -per_step_clip, per_step_clip)\n",
        "\n",
        "        # Compute grads on clamped log value.\n",
        "        grads_t = torch.autograd.grad(log_r_t_clamped, params, retain_graph=False, allow_unused=True)\n",
        "\n",
        "        # Accumulate detached grads.\n",
        "        for j, g in enumerate(grads_t):\n",
        "            if (g is not None) and torch.isfinite(g).all().item():\n",
        "                sum_grads[j].add_(g.detach())\n",
        "\n",
        "        # Numeric accumulation (sum of clamped logs).\n",
        "        log_r_sum += float(log_r_t_clamped.detach().cpu().item())\n",
        "\n",
        "        # Cleanup local tensors.\n",
        "        del x_t, x_tprev, eps_old, mu_old, eps_cur, mu_cur, diff_cur, diff_old, log_r_t, log_r_t_clamped, grads_t\n",
        "\n",
        "    # Clamp computed log_r_sum.\n",
        "    log_r_sum = max(min(log_r_sum, float(global_clip)), -float(global_clip))\n",
        "\n",
        "    return float(log_r_sum), sum_grads\n",
        "\n",
        "def L_CLIP_two_pass(old_model, cur_model, rm, alphas_cumprod, device, shape,\n",
        "                    ddim_steps, eta, eps_to_mu_fn, clip_eps, n_episodes,\n",
        "                    optimizer, grad_clip=1.0,\n",
        "                    microbatch=1, per_step_clip=5.0, global_clip=10.0, sigma_floor=1e-3):\n",
        "    \"\"\"\n",
        "    Run episodes and backpropagate the loss.\n",
        "    The process is consists of two passes.\n",
        "    1)\n",
        "\n",
        "    Parameters:\n",
        "        old_model (torch.nn.Module) : The fixed model updated in the previous epoch.\n",
        "        cur_model (torch.nn.Module) : The model updated most recently. Parameters of cur_model keeps updated in each episodes.\n",
        "        rm (torch.nn.Module) : Reward model.\n",
        "        alphas_cumprod (torch.Tensor) : Tensor of alpha_bars.\n",
        "        device (torch.device) : Device.\n",
        "        shape (tuple) : Shape of the image.\n",
        "        ddim_steps (int) : Number of steps in DDIM sampling process.\n",
        "        eta (float) : The ratio of probabilistic process in the DDPM(DDIM) reverse process.\n",
        "        eps_to_mu_fn (function) : Function to compute mu(mean) from epsilon.\n",
        "        clip_eps (float) : Clipping value of log(r_t)*A_t in the PPO.\n",
        "        n_episodes (int) : Number of episodes executed in each epoch.\n",
        "        optimizer (torch.nn.optim) : Optimizer to update parameters of cur_model.\n",
        "        grad_clip (float) :\n",
        "        microbatch (int) : Number of episodes executed in each epoch.\n",
        "        per_step_clip (float) : Clipping value of log r_t in each timestep.\n",
        "        global_clip (float) : Clipping value of log_r_sum.\n",
        "        simga_floor (float) : Minimum clipping value of sigma to avoid sigma to be too small.\n",
        "\n",
        "    Returns:\n",
        "        float : avg_loss_per_mb.\n",
        "        dictionary : stats. Has the information about the current epoch.\n",
        "    \"\"\"\n",
        "    # Get old_model, rm and fix their parameters.\n",
        "    old_model.eval()\n",
        "    rm.eval()\n",
        "    for p in old_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in rm.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Pass 1\n",
        "    buffers = [] # saves steps_log(t,x_t,eps_t) and x0_cpu in each episodes.\n",
        "    rewards_list = [] # saves total reward(float) in each episodes.\n",
        "\n",
        "    # Iter episodes.\n",
        "    for _ in range(n_episodes):\n",
        "        with torch.no_grad():\n",
        "            # Execute reverese process and get x0 and steps_log.\n",
        "            x0, steps_log = sample_ddim(\n",
        "                model=old_model, shape=shape,\n",
        "                alphas_cumprod=alphas_cumprod, device=device,\n",
        "                ddim_steps=ddim_steps, eta=eta\n",
        "            )\n",
        "\n",
        "            # Get steps_log_cpu, x0_cpu to save in the buffers list.\n",
        "            x0_cpu = x0.detach().cpu()\n",
        "            Tlist = steps_log[\"t\"]\n",
        "            Xlist = [xt.detach().cpu() for xt in steps_log[\"x_t\"]]\n",
        "            Elist = [et.detach().cpu() for et in steps_log[\"eps_t\"]]\n",
        "            steps_log_cpu = {\"t\": Tlist, \"x_t\": Xlist, \"eps_t\": Elist}\n",
        "\n",
        "            # Get reward from the trained reward model.\n",
        "            R_t = rm(x0_cpu.to(device)).mean()\n",
        "            R = float(R_t.item())\n",
        "\n",
        "        buffers.append((steps_log_cpu, x0_cpu))\n",
        "        rewards_list.append(R)\n",
        "\n",
        "    # Make it be a tensor.\n",
        "    rewards = torch.tensor(rewards_list, device=device, dtype=torch.float32)\n",
        "    advantages = rewards.detach()\n",
        "\n",
        "    # Prepare training.\n",
        "    cur_model.train()\n",
        "    total_loss_val = 0.0\n",
        "    clip_cnt = 0\n",
        "    sample_cnt = 0\n",
        "    valid_n = len(buffers)\n",
        "\n",
        "    # Pass 2\n",
        "    for i in range(0, valid_n, microbatch):\n",
        "        # Get microbatch number of episodes.\n",
        "        mb = buffers[i:i+microbatch]\n",
        "        adv_mb = advantages[i:i+microbatch]\n",
        "\n",
        "        # Set optimizer.\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        params = [p for p in cur_model.parameters() if p.requires_grad]\n",
        "\n",
        "        for (steps_log_cpu, x0_cpu), adv in zip(mb, adv_mb):\n",
        "            # Compute log_r_val and sum_grads in current microbatch.\n",
        "            log_r_val, sum_grads = compute_log_r_sum(\n",
        "                cur_model=cur_model,\n",
        "                steps_log=steps_log_cpu,\n",
        "                x0_cpu=x0_cpu,\n",
        "                alphas_cumprod=alphas_cumprod,\n",
        "                device=device,\n",
        "                eta=eta,\n",
        "                eps_to_mu_fn=eps_to_mu_fn,\n",
        "                per_step_clip=per_step_clip,\n",
        "                global_clip=global_clip,\n",
        "                sigma_floor=sigma_floor\n",
        "            )\n",
        "\n",
        "            # Get r_val and A(advantage).\n",
        "            r_val = math.exp(log_r_val)\n",
        "            A = float(adv.item())\n",
        "\n",
        "            # Clip r_val as in the PPO algorithm.\n",
        "            low, high = 1.0 - clip_eps, 1.0 + clip_eps\n",
        "            rc = min(max(r_val, low), high)  # rc : r_clipped\n",
        "\n",
        "            # loss = -min(r*A, rc*A)\n",
        "            val  = r_val * A # val denotes the value of objective function in PPO algorithm.\n",
        "            valc = rc    * A # valc : val_clipped\n",
        "            chosen = val if val < valc else valc\n",
        "            s = -chosen # We will use loss function, not an objective function, so multiply -1.\n",
        "\n",
        "            # Count the number of clipped episodes in current microbatch.\n",
        "            unclipped = (A >= 0 and r_val <= high) or (A < 0 and r_val >= low)\n",
        "            if not unclipped:\n",
        "                clip_cnt += 1\n",
        "            sample_cnt += 1\n",
        "\n",
        "            # Execute backpropagation only if the value is unclipped.\n",
        "            if unclipped:\n",
        "                # We want to compute (d/d_theta)(r_val)*A which is same as A*r_val*(d/d_theta)(log(r_val))\n",
        "                # The value of (d/d_theta)(log(r_val)) is sum_grads, computed in the function compute_log_r_sum.\n",
        "                scale = -A * r_val / float(microbatch)\n",
        "                for p, g in zip(params, sum_grads):\n",
        "                    grad_to_apply = (g.detach().clone().to(p.device)) * float(scale)\n",
        "                    if p.grad is None:\n",
        "                        p.grad = grad_to_apply\n",
        "                    else:\n",
        "                        p.grad.add_(grad_to_apply)\n",
        "\n",
        "        # Gradient clip and optimizer step.\n",
        "        total_norm = torch.nn.utils.clip_grad_norm_(cur_model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Return the log of single epoch.\n",
        "    clip_frac = (clip_cnt / sample_cnt) if sample_cnt > 0 else float(\"nan\")\n",
        "    stats = {\n",
        "        \"reward_mean\": float(rewards.mean().item()),\n",
        "        \"reward_std\": float(rewards.std(unbiased=False).item()),\n",
        "        \"episodes\": int(valid_n),\n",
        "        \"microbatch\": microbatch,\n",
        "        \"ddim\": ddim_steps,\n",
        "        \"eta\": eta,\n",
        "        \"clip\": clip_eps,\n",
        "        \"clip_frac\": float(clip_frac)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def single_epoch(ddpm_cur, ddpm_old, rm, alphas_cumprod, device, shape, optimizer,\n",
        "                 eps_to_mu_fn, ddim_steps=100, eta=1.0, clip_eps=0.1,\n",
        "                 episodes_per_epoch=30, normalize_rewards=True, grad_clip=1.0,\n",
        "                 microbatch=1):\n",
        "    \"\"\"\n",
        "    Run a single epoch.\n",
        "\n",
        "    Parameters:\n",
        "        ddpm_cur (torch.nn.Module) : The model updated most recently.\n",
        "        ddpm_old (torch.nn.Module) : The fixed model updated in the previous epoch.\n",
        "        rm (torch.nn.Module) : Reward model.\n",
        "        alphas_cumprod (torch.Tensor) : Tensor of alpha_bars.\n",
        "        device (torch.device) : Device.\n",
        "        shape (tuple) : Shape of the image.\n",
        "        optimizer (torch.nn.optim) : Optimizer to update parameters of cur_model.\n",
        "        eps_to_mu_fn (function) : Function to compute mu(mean) from epsilon.\n",
        "        ddim_steps (int) : Number of steps in DDIM sampling process.\n",
        "        eta (float) : The ratio of probabilistic process in the DDPM(DDIM) reverse process.\n",
        "        clip_eps (float) : Clipping value of log(r_t)*A_t in the PPO\n",
        "        episodes_per_epoch (int) : Number of episodes executed in each epoch.\n",
        "        normalize_rewards (bool) : Whether to normalize rewards.\n",
        "        grad_clip (float) :\n",
        "        microbatch (int) : Number of episodes executed in each epoch.\n",
        "\n",
        "    Returns:\n",
        "        dictionary : stats. Has the information about the current epoch.\n",
        "    \"\"\"\n",
        "    ddpm_old.eval()\n",
        "    for p in ddpm_old.parameters(): p.requires_grad = False\n",
        "    rm.eval()\n",
        "    for p in rm.parameters(): p.requires_grad = False\n",
        "\n",
        "    stats = L_CLIP_two_pass(\n",
        "        old_model=ddpm_old,\n",
        "        cur_model=ddpm_cur,\n",
        "        rm=rm,\n",
        "        alphas_cumprod=alphas_cumprod,\n",
        "        device=device,\n",
        "        shape=shape,\n",
        "        ddim_steps=ddim_steps,\n",
        "        eta=eta,\n",
        "        eps_to_mu_fn=eps_to_mu_fn,\n",
        "        clip_eps=clip_eps,\n",
        "        n_episodes=episodes_per_epoch,\n",
        "        optimizer=optimizer,\n",
        "        grad_clip=grad_clip,\n",
        "        microbatch=microbatch\n",
        "    )\n",
        "    stats.update({\"ddim_steps\": ddim_steps, \"eta\": eta, \"clip_eps\": clip_eps})\n",
        "    return stats"
      ],
      "metadata": {
        "id": "flJDCgbfXLWi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train setup"
      ],
      "metadata": {
        "id": "TpYT7RQvYsTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ddpm_cur = DDPMModel().to(device)\n",
        "ckpt_path = '/content/drive/MyDrive/cv프로젝트/부트캠프/_100.pth'\n",
        "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "state_dict = checkpoint[\"model_state_dict\"] if \"model_state_dict\" in checkpoint else checkpoint\n",
        "ddpm_cur.load_state_dict(state_dict)\n",
        "print(f\"Loaded base DDPM from {ckpt_path}\")\n",
        "\n",
        "ddpm_old = copy.deepcopy(ddpm_cur).to(device)\n",
        "for p in ddpm_old.parameters():\n",
        "    p.requires_grad = False\n",
        "ddpm_old.eval()\n",
        "\n",
        "rm = RewardModel().to(device)\n",
        "rm_ckpt_path = '/content/drive/MyDrive/논문코드리뷰/논문코드리뷰_PPO/reward_model_v2.pt'\n",
        "rm_state = torch.load(rm_ckpt_path, map_location=\"cpu\")\n",
        "rm.load_state_dict(rm_state[\"model_state_dict\"] if \"model_state_dict\" in rm_state else rm_state)\n",
        "rm.eval()\n",
        "for p in rm.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "betas, alphas, alpha_bars = get_beta_alpha_linear()\n",
        "alphas = alphas.to(device)\n",
        "alpha_bars = alpha_bars.to(device)\n",
        "\n",
        "eps_to_mu_fn = lambda x_t, t_idx, eps_pred: compute_mu_from_eps(\n",
        "    x_t, t_idx, eps_pred, alphas, alpha_bars\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(ddpm_cur.parameters(), lr=1e-6)\n",
        "\n",
        "# Parameters\n",
        "epochs               = 10\n",
        "episodes_per_epoch   = 10\n",
        "ddim_steps           = 200\n",
        "eta                  = 0.5 # Sampling process is not deterministic.\n",
        "clip_eps             = 0.2\n",
        "normalize_rewards    = False\n",
        "grad_clip            = 1.0\n",
        "microbatch           = 1\n",
        "\n",
        "# Shape\n",
        "B = 1\n",
        "C = ddpm_cur.in_channels\n",
        "H = W = ddpm_cur.init_resolution\n",
        "shape = (B, C, H, W)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_with_cur_model(cur_model, rm, alphas_cumprod, device, ddim_steps=200, eta=0.5, n_eval=8):\n",
        "    # Get rewards of the image generated by cur_model.\n",
        "    cur_model.eval()\n",
        "    x0, _ = sample_ddim(model=cur_model, shape=(8,3,64,64), alphas_cumprod=alphas_cumprod,\n",
        "                        device=device, ddim_steps=ddim_steps, eta=eta)\n",
        "    return rm(x0).mean().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBm0YECfKN5M",
        "outputId": "c3323515-636f-468b-e788-0ccdaadb27a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loaded base DDPM from /content/drive/MyDrive/cv프로젝트/부트캠프/_100.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "lcok62SE7OnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(1, epochs + 1)):\n",
        "    # Execute single epoch.\n",
        "    stats = single_epoch(\n",
        "        ddpm_cur=ddpm_cur,\n",
        "        ddpm_old=ddpm_old,\n",
        "        rm=rm,\n",
        "        alphas_cumprod=alpha_bars,\n",
        "        device=device,\n",
        "        shape=shape,\n",
        "        optimizer=optimizer,\n",
        "        eps_to_mu_fn=eps_to_mu_fn,\n",
        "        ddim_steps=ddim_steps,\n",
        "        eta=eta,\n",
        "        clip_eps=clip_eps,\n",
        "        episodes_per_epoch=episodes_per_epoch,\n",
        "        grad_clip=grad_clip,\n",
        "        microbatch=microbatch\n",
        "    )\n",
        "\n",
        "    # Compute reward of the image generated by recently updated model.\n",
        "    cur_eval_mean = eval_with_cur_model(ddpm_cur, rm, alpha_bars, device,\n",
        "                                                      ddim_steps=ddim_steps, eta=eta, n_eval=8)\n",
        "\n",
        "    # Print log.\n",
        "    print(f\"[Epoch {epoch}/{epochs}] \"\n",
        "          f\"reward_mean(old)={stats['reward_mean']:.4f} ± {stats['reward_std']:.4f} | \"\n",
        "          f\"cur_eval={cur_eval_mean:.4f} | \"\n",
        "          f\"clip_frac={stats.get('clip_frac', float('nan')):.2f} | \"\n",
        "          f\"episodes={stats['episodes']} | mb={stats['microbatch']} | \"\n",
        "          f\"ddim={stats['ddim_steps']} | eta={stats['eta']} | clip={stats['clip_eps']} | \")\n",
        "\n",
        "    # Set ddpm_old as ddpm_cur and fix it for the next epoch.\n",
        "    ddpm_old.load_state_dict(ddpm_cur.state_dict())\n",
        "    ddpm_old.eval()\n",
        "    for p in ddpm_old.parameters():\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "de127a46e1da4a8a96e1a833dfa39a61",
            "eee08f14fdf74695bf1e5edd40a46864",
            "ae3ba473ba9b47a987677369909ad8c6",
            "9d029a532b014bb0a7f1584dd343f426",
            "76127f30451d46b29473ac2a213c4cfd",
            "2736e8d297b545c09d7e8c6ead7a6d37",
            "9c83b461aa224cc3a8c60b73c49a6565",
            "381ec5457fbf4ccd867d068f4f0428f4",
            "3a9c5c8ec95d4b51acebdbeb489abb18",
            "21332c05f06c44da91d7f443a4824c09",
            "59bda4aee9164b16922514013ca4e28c"
          ]
        },
        "id": "KcJefcMJ7N3V",
        "outputId": "dac2cc43-bc13-4343-9de4-46ae2af5ee43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de127a46e1da4a8a96e1a833dfa39a61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/10] reward_mean(old)=0.6976 ± 0.5925 | cur_eval=-0.4406 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 2/10] reward_mean(old)=-0.0157 ± 0.5406 | cur_eval=0.9784 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 3/10] reward_mean(old)=0.3404 ± 0.7218 | cur_eval=-0.2075 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 4/10] reward_mean(old)=0.1421 ± 0.9821 | cur_eval=0.3079 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 5/10] reward_mean(old)=0.5552 ± 0.7723 | cur_eval=0.3439 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 6/10] reward_mean(old)=0.6256 ± 0.9455 | cur_eval=0.5896 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 7/10] reward_mean(old)=-0.1001 ± 0.7877 | cur_eval=0.2642 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 8/10] reward_mean(old)=0.0699 ± 0.7301 | cur_eval=0.6921 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 9/10] reward_mean(old)=0.4544 ± 0.8626 | cur_eval=0.4939 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n",
            "[Epoch 10/10] reward_mean(old)=0.5063 ± 1.0931 | cur_eval=-0.0788 | clip_frac=0.00 | episodes=10 | mb=1 | ddim=200 | eta=0.5 | clip=0.2 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show\n",
        "Human image is well generated also by finetuned DDPM model."
      ],
      "metadata": {
        "id": "v74QJY1naoWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddpm_cur.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x0,_ = sample_ddim(\n",
        "        model=ddpm_cur,\n",
        "        shape=shape,\n",
        "        alphas_cumprod=alpha_bars,\n",
        "        device=device,\n",
        "        ddim_steps=ddim_steps,\n",
        "        eta=eta\n",
        "    )\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def visualize_sample(x0, idx=0):\n",
        "    img = x0[idx]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = (img + 1.0) / 2.0\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Sampled x_0\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_sample(x0.cpu(),idx=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "PghJWuC0aaaR",
        "outputId": "9b15ee37-d841-47ff-ef06-355ff00a53c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Reward\n",
        "Finetuned model attains higher reward than trained DDPM model."
      ],
      "metadata": {
        "id": "IINJD4XqkboV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddpm_old = DDPMModel().to(device)\n",
        "ckpt_path = '/content/drive/MyDrive/cv프로젝트/부트캠프/_100.pth'\n",
        "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "state_dict = checkpoint[\"model_state_dict\"] if \"model_state_dict\" in checkpoint else checkpoint\n",
        "ddpm_old.load_state_dict(state_dict)\n",
        "\n",
        "ddpm_old.eval()\n",
        "ddpm_cur.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x0_old,_ = sample_ddim(\n",
        "        model=ddpm_old,\n",
        "        shape=(64,3,64,64),\n",
        "        alphas_cumprod=alpha_bars,\n",
        "        device=device,\n",
        "        ddim_steps=ddim_steps,\n",
        "        eta=eta\n",
        "    )\n",
        "    x0_cur,_ = sample_ddim(\n",
        "        model=ddpm_cur,\n",
        "        shape=(64,3,64,64),\n",
        "        alphas_cumprod=alpha_bars,\n",
        "        device=device,\n",
        "        ddim_steps=ddim_steps,\n",
        "        eta=eta\n",
        "    )\n",
        "\n",
        "    r_old = rm(x0_old).mean().item()\n",
        "    r_cur = rm(x0_cur).mean().item()\n",
        "    print(\"old reward:\", r_old)\n",
        "    print(\"cur reward:\", r_cur)"
      ],
      "metadata": {
        "id": "compareRewardCell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
