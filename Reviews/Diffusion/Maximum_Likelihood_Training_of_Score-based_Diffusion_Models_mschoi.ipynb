{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "K9gxl4MBvLHj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Paper : \"Maximum Likelihood Training of Score-based Generative Diffusion Models\" by Yang Song et al. (2021)\n",
        "# The code below was written with reference to the paper's official open source code.\n",
        "# Github Repository : https://github.com/yang-song/score_sde"
      ],
      "metadata": {
        "id": "OU3gWP8vuxUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "jdpCW7YevJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import json\n",
        "import math"
      ],
      "metadata": {
        "id": "DHIEBJDedkwg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "K9gxl4MBvLHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Gaussian Fourier embeddings for continuous noise levels.\n",
        "\n",
        "    Parameters:\n",
        "        embedding_size (int) : Size of embedding\n",
        "        scale (float) : Scaling factor\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data. Get timestep t from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Gaussian Fourier embeddings\n",
        "\n",
        "        Example:\n",
        "            >>> embedding_size = 64\n",
        "            >>> scale = 1.0\n",
        "            >>> fourier = GaussianFourierProjection(embedding_size, scale)\n",
        "            >>> t = torch.randn(64)\n",
        "            >>> out = fourier(t)\n",
        "            >>> print(out.shape) # torch.Size([64,128])\n",
        "        \"\"\"\n",
        "        x_proj = x.unsqueeze(1) * self.W.unsqueeze(0) * 2 * math.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "def swish(x):\n",
        "    # Activation function used in DDPM\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    # GroupNorm 8 is used in the model instead of GroupNorm 32\n",
        "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    # 1x1 convolution\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    # Block that doubles down on resolution. Use convolution block or average pooling block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    # Block that doubles the resolution. Use convolution block or interpolating block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet block used in DDPM. Use group normalization. Continuous timestep t is also received as input.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of Output channels\n",
        "        temb_channels (int) : Number of time embedding channels\n",
        "        dropout (float) : Dropout rate\n",
        "        conv_shortcut (bool) : Whether to add convolution shortcut\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None,\n",
        "                 temb_channels=512, dropout=0.0, conv_shortcut=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = GroupNorm32(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm32(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.conv_shortcut:\n",
        "                self.conv_shortcut = nn.Conv2d(self.in_channels, self.out_channels,\n",
        "                                               kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "        else:\n",
        "            self.conv_shortcut = None\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            temb (torch.Tensor) : Embedding vector of timestep t\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,3,32,32)\n",
        "            >>> temb = torch.randn(128,64)\n",
        "            >>> block = ResnetBlock(in_channels=3,out_channels=32,temb_channels=64)\n",
        "            >>> out = block(x,temb)\n",
        "            >>> print(out.shape) # torch.Size([128,32,32,32])\n",
        "        \"\"\"\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h) # B*3*32*32\n",
        "        h = self.conv1(h) # B*32*32*32\n",
        "        h_temb = swish(temb) # B*64\n",
        "        h_temb = self.temb_proj(h_temb)  # B*32\n",
        "        h_temb = h_temb[:, :, None, None]  # B*32*1*1\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h) # B*32*32*32\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h) # B*32*32*32\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x) # B*32*32*32\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention block used in DDPM.\n",
        "\n",
        "    Parameters:\n",
        "        channels (int) : Number of input, output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm32(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,32,224,224)\n",
        "            >>> block = AttnBlock(32)\n",
        "            >>> out = block(x)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h).permute(0, 2, 3, 1)  # (B, H, W, C)\n",
        "        k = self.k(h).permute(0, 2, 3, 1)\n",
        "        v = self.v(h).permute(0, 2, 3, 1)\n",
        "        w = torch.einsum('bhwc,bHWc->bhwHW', q, k) * (C ** -0.5)\n",
        "        w = w.reshape(B, H, W, H * W)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = w.reshape(B, H, W, H, W)\n",
        "        h_ = torch.einsum('bhwHW,bHWc->bhwc', w, v)\n",
        "        h_ = self.proj_out(h_.permute(0, 3, 1, 2))\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Final DDPMModel. Modified version of UNet.\n",
        "    Attention block is applied to where it is set to applied.\n",
        "    Given noised data x_t and timestep t, the model estimates the value of noise at timestep t.\n",
        "    This DDPM model is same as the one used in the DDPM paper except for the GaussianFourierProjection.\n",
        "    Unlike origianl DDPM model, timestep t is set to [0,1] in this model.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of output channels\n",
        "        ch (int) : Number of default channel\n",
        "        ch_mult (tuple) : Coefficient multiblied by channels\n",
        "        num_res_blocks (int) : Number of resnet blocks\n",
        "        attn_resolutions (set) : Set of resolutions at which attention block applies\n",
        "        dropout (float) : Dropout rate\n",
        "        resamp_with_conv (bool) : Whether to use convolution while down(up)sampling\n",
        "        init_resolution (int) : Resolution of input images\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 2, 4),\n",
        "        num_res_blocks=3,\n",
        "        attn_resolutions={28,56},\n",
        "        dropout=0.1,\n",
        "        resamp_with_conv=True,\n",
        "        init_resolution=224\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "\n",
        "        # Time Embedding\n",
        "        self.fourier = GaussianFourierProjection(embedding_size=self.ch, scale=1.0)\n",
        "\n",
        "        # Dimension of time embedding vector\n",
        "        self.temb_ch = ch * 4\n",
        "\n",
        "        # Timestep embedding layers\n",
        "        self.temb_dense0 = linear(2*self.ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "\n",
        "        # Input conv\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Downsample blocks\n",
        "        # Each downsampling block(modulelist) is stored in down_blocks.\n",
        "        # Each block is composed of resnetblocks and attention block(if needed).\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        # Symmetric structure with downsample blocks\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "\n",
        "        # output conv\n",
        "        self.norm_out = GroupNorm32(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            sigma (torch.Tensor) : Timesteps of batch data. Get continuous noise level sigma(t) from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            x = torch.randn(128,3,32,32)\n",
        "            t = torch.rand(128)\n",
        "            model = DDPMModel(ch=64,attn_resolution={16},init_resolution=32)\n",
        "            out = model(x,t)\n",
        "            print(out.shape) # torch.Size([128,3,32,32])\n",
        "        \"\"\"\n",
        "        # 1) Timestep embedding\n",
        "        log_sigma = torch.log(sigma)\n",
        "        temb = self.fourier(log_sigma) # B*2ch\n",
        "        temb = self.temb_dense0(temb) # B*4ch\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb) # B*4ch\n",
        "\n",
        "        # 2) Downsampling\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "\n",
        "        # 3) Middle\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "\n",
        "        # 4) Upsampling\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "\n",
        "        # 5) Output\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "class VESDE:\n",
        "    \"\"\"\n",
        "    Variance Exploding SDE.\n",
        "\n",
        "    Parameters:\n",
        "        sigma_min (float) : Minimum sigma\n",
        "        sigma_max (float) : Maximum sigma\n",
        "        N (int) : Number of discretization steps\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma_min=0.01, sigma_max=50.0, N=1000):\n",
        "        self.smin, self.smax, self.N = sigma_min, sigma_max, N\n",
        "        self.T = 1.0\n",
        "        self.log_smin = math.log(self.smin)\n",
        "        self.log_smax = math.log(self.smax)\n",
        "\n",
        "    def sigma(self, t):\n",
        "        # t in [0,1]\n",
        "        return torch.exp(self.log_smin + t * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def marginal_prob(self, x0, t):\n",
        "        # mean = x0, std = sigma(t)\n",
        "        std = self.sigma(t).view(-1,1,1,1)\n",
        "        return x0, std\n",
        "\n",
        "    def diffusion_coeff(self, t):\n",
        "        # g(t) in VESDE\n",
        "        return self.sigma(t) * math.sqrt(2 * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def sample_prior(self, shape, device):\n",
        "        # Return a sample from the prior distribution.\n",
        "        return torch.randn(shape, device=device) * self.smax"
      ],
      "metadata": {
        "id": "tef7cC6Zq7sc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "KhyhsOUDvOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DDPMModel(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    ch=32,\n",
        "    ch_mult=(1, 2, 4),\n",
        "    num_res_blocks=2,\n",
        "    attn_resolutions={16},\n",
        "    dropout=0.1,\n",
        "    resamp_with_conv=True,\n",
        "    init_resolution=32\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "sde = VESDE(sigma_min=0.01, sigma_max=50.0, N=1000)\n",
        "\n",
        "model.train()\n",
        "for i in tqdm(range(20)):\n",
        "    loss_list = []\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        B = x.size(0)\n",
        "        t = torch.rand(B, device=device)\n",
        "        mean,std = sde.marginal_prob(x,t)\n",
        "        z = torch.randn_like(x)\n",
        "        xt = mean + std * z\n",
        "\n",
        "        score = model(xt, std.view(B))\n",
        "        res = score + z / std\n",
        "\n",
        "        # Apply likelihood weighting. Let lambda(t)=g(t)^2\n",
        "        g = sde.diffusion_coeff(t).view(B, 1, 1, 1)\n",
        "        loss = (res.pow(2) * g.pow(2)).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss = np.mean(loss_list).item()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f'Epoch: {i}, Loss: {avg_loss:.4f} lr: {current_lr:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430,
          "referenced_widgets": [
            "9cbbdc8529a146229399516d137ac51e",
            "ab90ceddad3e4375b35fb1c30e4d19dd",
            "ad1ea008703140849b771a9d969ac9d4",
            "01a1eb996c5c4a9ba27202f88d469fc5",
            "fbaf25adb8c847e0abb485322310e5a6",
            "c858fdf8f4bc4347ae55cfb691ef1997",
            "c521b4f990334bd7a7006f1a4a26c889",
            "cb60c4474f54483880e3d65805798607",
            "6648d0411ff94bbba711b2183ccefd7a",
            "102d0c166652409496f32f2cc28d5f04",
            "0f116297e3984912a45bcad7a9377d71"
          ]
        },
        "id": "HWjC-b8XeVpq",
        "outputId": "506d2a24-3333-4202-af29-3062c5f4ef46",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cbbdc8529a146229399516d137ac51e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 9.5560 lr: 0.0020\n",
            "Epoch: 1, Loss: 5.6000 lr: 0.0020\n",
            "Epoch: 2, Loss: 4.7577 lr: 0.0020\n",
            "Epoch: 3, Loss: 3.9806 lr: 0.0020\n",
            "Epoch: 4, Loss: 3.5889 lr: 0.0020\n",
            "Epoch: 5, Loss: 3.3010 lr: 0.0020\n",
            "Epoch: 6, Loss: 3.1112 lr: 0.0020\n",
            "Epoch: 7, Loss: 2.9950 lr: 0.0020\n",
            "Epoch: 8, Loss: 5.5378 lr: 0.0020\n",
            "Epoch: 9, Loss: 3.0378 lr: 0.0002\n",
            "Epoch: 10, Loss: 2.8465 lr: 0.0002\n",
            "Epoch: 11, Loss: 2.7770 lr: 0.0002\n",
            "Epoch: 12, Loss: 2.7518 lr: 0.0002\n",
            "Epoch: 13, Loss: 2.7192 lr: 0.0002\n",
            "Epoch: 14, Loss: 2.7107 lr: 0.0002\n",
            "Epoch: 15, Loss: 2.6822 lr: 0.0002\n",
            "Epoch: 16, Loss: 2.6645 lr: 0.0002\n",
            "Epoch: 17, Loss: 2.6562 lr: 0.0002\n",
            "Epoch: 18, Loss: 2.6439 lr: 0.0002\n",
            "Epoch: 19, Loss: 2.6406 lr: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "lAVYVSHgvPuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def pc_sampler(model, sde, shape, device, snr=0.16, n_steps_pc=1):\n",
        "    \"\"\"\n",
        "    Predictor–Corrector sampler compatible with our DDPMModel and VESDE.\n",
        "    Refer to Algorithm 1,2,4 at Appendix F,G.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module) : Trained DDPMModel\n",
        "        sde : SDE object (e.g. VESDE) with methods\n",
        "        shape (tuple) : Shape of sampled image\n",
        "        device (torch.device) : Device\n",
        "        snr (float) : Signal to noise ratio\n",
        "        n_steps_pc (int) : Number of predictor-corrector steps\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Sampled image\n",
        "\n",
        "    Example:\n",
        "        >>> model = DDPMModel(ch=64,attn_resolution={16},init_resolution=32)\n",
        "        >>> sde = VESDE(sigma_min=0.01,sigma_max=50.0,N=1000)\n",
        "        >>> sample = pc_sampler(model,sde,shape=(64,3,32,32),device=device)\n",
        "        >>> print(sample.shape) # torch.Size([64,3,32,32])\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = sde.sample_prior(shape, device)\n",
        "    t_steps = torch.linspace(sde.T, 1e-3, sde.N, device=device)\n",
        "    dt = -(sde.T - 1e-3) / (sde.N - 1) # Each reverse step is adjusted to (0,1)\n",
        "\n",
        "    for t in tqdm(t_steps):\n",
        "        batch_t = t.repeat(shape[0])\n",
        "        sigma = sde.sigma(batch_t)\n",
        "\n",
        "        # Corrector (Langevin Dynamics)\n",
        "        for _ in range(n_steps_pc):\n",
        "            z = torch.randn_like(x)\n",
        "            score = model(x, sigma)  # gradient log p_t(x)\n",
        "            noise_norm = torch.norm(z.view(shape[0], -1), dim=1)\n",
        "            score_norm = torch.norm(score.view(shape[0], -1), dim=1)\n",
        "            step_size = (snr * noise_norm / (score_norm + 1e-12))**2\n",
        "            step_size = step_size.view(-1,1,1,1)\n",
        "            # Langevin update\n",
        "            x = x + step_size * score + torch.sqrt(2 * step_size) * z\n",
        "\n",
        "        # Predictor (Euler–Maruyama for the reverse SDE)\n",
        "        z = torch.randn_like(x)\n",
        "        g = sde.diffusion_coeff(t)\n",
        "        score = model(x, sigma)\n",
        "        # reverse SDE drift = -g^2 * score\n",
        "        x = x + (-g**2 * score) * dt + g * math.sqrt(-dt) * z\n",
        "\n",
        "    return x.clamp(-1,1)"
      ],
      "metadata": {
        "id": "LoD7432vh-I4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "q7aGTwqsvRjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(x0, idx=0):\n",
        "    \"\"\"\n",
        "    Visualize tensor data which is in the range of -1 to 1.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Tensor data\n",
        "        idx (int) : Index of tensor data ('idx'th image in the batch)\n",
        "    \"\"\"\n",
        "    img = x0[idx]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = (img + 1.0) / 2.0\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Sampled x_0\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qZi9-XqukRQ9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create samples"
      ],
      "metadata": {
        "id": "AxX_8HmL3t91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pc_sampler(model, sde, shape=(64,3,32,32), device=device, n_steps_pc=3)\n",
        "visualize_sample(sample.cpu().detach().numpy(), idx=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "eb55b849a9c749bca5b2b0fc1084d435",
            "bb55d71d74754a5fa1b6d21c3f8b20fc",
            "3bd3f4ddcf86440fa0f09964cd6be231",
            "ec12c7d6cf7149ab89d2c822658a29a8",
            "4b4cafb6580947ff92a27eb96584d3af",
            "981e1854b56246faa17ab09666c8e360",
            "1e97f83ba582430f8639a9e643ba5ad2",
            "768fdb5d04a14e039fa8027c8c69688a",
            "b80a4d46d2434cbe87b65a46ffd822e9",
            "fe21daae1e3943ec9939910b45316b29",
            "78cc8e5d479c49db98d6f0915af9b3be"
          ]
        },
        "id": "SpjOufDYj7XD",
        "outputId": "d6f84cf8-e2e7-4778-90df-a17b5f9468f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb55b849a9c749bca5b2b0fc1084d435"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
