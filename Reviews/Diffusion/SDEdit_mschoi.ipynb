{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vqoxeJj_KbUs",
        "Qn3NBbQWKjAE",
        "A-0PHZq7Kqpz",
        "M7haizm6NiZN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3dm0lb4H4Zu"
      },
      "outputs": [],
      "source": [
        "# Paper : \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\" by Chenlin Meng et al. (2022)\n",
        "# The code below was written with reference to the paper's official open source code.\n",
        "# Github Repository : https://github.com/ermongroup/SDEdit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "vqoxeJj_KbUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "MdmxtkJLJ1Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Qn3NBbQWKjAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Gaussian Fourier embeddings for continuous noise levels.\n",
        "\n",
        "    Parameters:\n",
        "        embedding_size (int) : Size of embedding\n",
        "        scale (float) : Scaling factor\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data. Get timestep t from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Gaussian Fourier embeddings\n",
        "\n",
        "        Example:\n",
        "            >>> embedding_size = 64\n",
        "            >>> scale = 1.0\n",
        "            >>> fourier = GaussianFourierProjection(embedding_size, scale)\n",
        "            >>> t = torch.randn(64)\n",
        "            >>> out = fourier(t)\n",
        "            >>> print(out.shape) # torch.Size([64,128])\n",
        "        \"\"\"\n",
        "        x_proj = x.unsqueeze(1) * self.W.unsqueeze(0) * 2 * math.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "def swish(x):\n",
        "    # Activation function used in DDPM\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    # GroupNorm 8 is used in the model instead of GroupNorm 32\n",
        "    def __init__(self, num_channels, num_groups=8, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    # 1x1 convolution\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    # Block that doubles down on resolution. Use convolution block or average pooling block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    # Block that doubles the resolution. Use convolution block or interpolating block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet block used in DDPM. Use group normalization. Continuous timestep t is also received as input.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of Output channels\n",
        "        temb_channels (int) : Number of time embedding channels\n",
        "        dropout (float) : Dropout rate\n",
        "        conv_shortcut (bool) : Whether to add convolution shortcut\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None,\n",
        "                 temb_channels=512, dropout=0.0, conv_shortcut=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = GroupNorm32(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm32(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.conv_shortcut:\n",
        "                self.conv_shortcut = nn.Conv2d(self.in_channels, self.out_channels,\n",
        "                                               kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "        else:\n",
        "            self.conv_shortcut = None\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            temb (torch.Tensor) : Embedding vector of timestep t\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,3,32,32)\n",
        "            >>> temb = torch.randn(128,64)\n",
        "            >>> block = ResnetBlock(in_channels=3,out_channels=32,temb_channels=64)\n",
        "            >>> out = block(x,temb)\n",
        "            >>> print(out.shape) # torch.Size([128,32,32,32])\n",
        "        \"\"\"\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h) # B*3*32*32\n",
        "        h = self.conv1(h) # B*32*32*32\n",
        "        h_temb = swish(temb) # B*64\n",
        "        h_temb = self.temb_proj(h_temb)  # B*32\n",
        "        h_temb = h_temb[:, :, None, None]  # B*32*1*1\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h) # B*32*32*32\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h) # B*32*32*32\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x) # B*32*32*32\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention block used in DDPM.\n",
        "\n",
        "    Parameters:\n",
        "        channels (int) : Number of input, output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm32(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,32,224,224)\n",
        "            >>> block = AttnBlock(32)\n",
        "            >>> out = block(x)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h).permute(0, 2, 3, 1)  # (B, H, W, C)\n",
        "        k = self.k(h).permute(0, 2, 3, 1)\n",
        "        v = self.v(h).permute(0, 2, 3, 1)\n",
        "        w = torch.einsum('bhwc,bHWc->bhwHW', q, k) * (C ** -0.5)\n",
        "        w = w.reshape(B, H, W, H * W)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = w.reshape(B, H, W, H, W)\n",
        "        h_ = torch.einsum('bhwHW,bHWc->bhwc', w, v)\n",
        "        h_ = self.proj_out(h_.permute(0, 3, 1, 2))\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Final DDPMModel. Modified version of UNet.\n",
        "    Attention block is applied to where it is set to applied.\n",
        "    Given noised data x_t and timestep t, the model estimates the value of noise at timestep t.\n",
        "    This DDPM model is same as the one used in the DDPM paper except for the GaussianFourierProjection.\n",
        "    Unlike origianl DDPM model, timestep t is set to [0,1] in this model.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of output channels\n",
        "        ch (int) : Number of default channel\n",
        "        ch_mult (tuple) : Coefficient multiblied by channels\n",
        "        num_res_blocks (int) : Number of resnet blocks\n",
        "        attn_resolutions (set) : Set of resolutions at which attention block applies\n",
        "        dropout (float) : Dropout rate\n",
        "        resamp_with_conv (bool) : Whether to use convolution while down(up)sampling\n",
        "        init_resolution (int) : Resolution of input images\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 2, 4),\n",
        "        num_res_blocks=3,\n",
        "        attn_resolutions={28,56},\n",
        "        dropout=0.1,\n",
        "        resamp_with_conv=True,\n",
        "        init_resolution=224\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "\n",
        "        # Time Embedding\n",
        "        self.fourier = GaussianFourierProjection(embedding_size=self.ch, scale=1.0)\n",
        "\n",
        "        # Dimension of time embedding vector\n",
        "        self.temb_ch = ch * 4\n",
        "\n",
        "        # Timestep embedding layers\n",
        "        self.temb_dense0 = linear(2*self.ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "\n",
        "        # Input conv\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Downsample blocks\n",
        "        # Each downsampling block(modulelist) is stored in down_blocks.\n",
        "        # Each block is composed of resnetblocks and attention block(if needed).\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        # Symmetric structure with downsample blocks\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "\n",
        "        # output conv\n",
        "        self.norm_out = GroupNorm32(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            sigma (torch.Tensor) : Timesteps of batch data. Get continuous noise level sigma(t) from [0,1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            x = torch.randn(128,3,32,32)\n",
        "            t = torch.rand(128)\n",
        "            model = DDPMModel(ch=64,attn_resolution={16},init_resolution=32)\n",
        "            out = model(x,t)\n",
        "            print(out.shape) # torch.Size([128,3,32,32])\n",
        "        \"\"\"\n",
        "        # 1) Timestep embedding\n",
        "        log_sigma = torch.log(sigma)\n",
        "        temb = self.fourier(log_sigma) # B*2ch\n",
        "        temb = self.temb_dense0(temb) # B*4ch\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb) # B*4ch\n",
        "\n",
        "        # 2) Downsampling\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "\n",
        "        # 3) Middle\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "\n",
        "        # 4) Upsampling\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "\n",
        "        # 5) Output\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "class VESDE:\n",
        "    \"\"\"\n",
        "    Variance Exploding SDE.\n",
        "\n",
        "    Parameters:\n",
        "        sigma_min (float) : Minimum sigma\n",
        "        sigma_max (float) : Maximum sigma\n",
        "        N (int) : Number of discretization steps\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma_min=0.01, sigma_max=50.0, N=1000):\n",
        "        self.smin, self.smax, self.N = sigma_min, sigma_max, N\n",
        "        self.T = 1.0\n",
        "        self.log_smin = math.log(self.smin)\n",
        "        self.log_smax = math.log(self.smax)\n",
        "\n",
        "    def sigma(self, t):\n",
        "        # t in [0,1]\n",
        "        return torch.exp(self.log_smin + t * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def marginal_prob(self, x0, t):\n",
        "        # mean = x0, std = sigma(t)\n",
        "        std = self.sigma(t).view(-1,1,1,1)\n",
        "        return x0, std\n",
        "\n",
        "    def diffusion_coeff(self, t):\n",
        "        # g(t) in VESDE\n",
        "        return self.sigma(t) * math.sqrt(2 * (self.log_smax - self.log_smin))\n",
        "\n",
        "    def sample_prior(self, shape, device):\n",
        "        # Return a sample from the prior distribution.\n",
        "        return torch.randn(shape, device=device) * self.smax"
      ],
      "metadata": {
        "id": "z-lcpTIVJ1JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampler"
      ],
      "metadata": {
        "id": "A-0PHZq7Kqpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sdedit_sampler(model, sde, x_guidance, t0, device, snr=0.16, n_steps_pc=1):\n",
        "    \"\"\"\n",
        "    SDEdit sampler (Predictor–Corrector)\n",
        "    Starts from timestep t0 and proceed with predictor-corrector sampler.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module) : Trained scorenet or DDPM model\n",
        "        sde (SDE) : SDE object\n",
        "        x_guidance (torch.Tensor) : torch.Size([B, C, H, W])\n",
        "        t0 (float) : Start timestep (0 < t0 ≤ sde.T)\n",
        "        device (torch.device) : device\n",
        "        snr\n",
        "        n_steps_pc (int) : Number of Predictor–Corrector steps\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : torch.Size([B, C, H, W])\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = x_guidance.to(device).clone()\n",
        "    B = x.shape[0]\n",
        "\n",
        "    # Insert noise to the guide image\n",
        "    t0_batch = torch.ones(B, device=device) * t0\n",
        "    sigma0 = sde.sigma(t0_batch)\n",
        "    z0 = torch.randn_like(x)\n",
        "    x = x + z0 * sigma0.view(B,1,1,1)\n",
        "\n",
        "    # Set reverse SDE timesteps\n",
        "    eps = 1e-3\n",
        "    timesteps = torch.linspace(t0, eps, sde.N, device=device)\n",
        "    dt = -(t0 - eps) / (sde.N - 1)\n",
        "\n",
        "    # Predictor–Corrector loop\n",
        "    for t in tqdm(timesteps):\n",
        "        batch_t = t.repeat(B)\n",
        "        sigma = sde.sigma(batch_t)\n",
        "\n",
        "        # Corrector: Langevin Dynamics\n",
        "        for _ in range(n_steps_pc):\n",
        "            noise = torch.randn_like(x)\n",
        "            score = model(x, sigma)\n",
        "            noise_norm = noise.view(B, -1).norm(dim=1)\n",
        "            score_norm = score.view(B, -1).norm(dim=1)\n",
        "            step_size = (snr * noise_norm / (score_norm + 1e-12))**2\n",
        "            step_size = step_size.view(B,1,1,1)\n",
        "            x = x + step_size * score + torch.sqrt(2 * step_size) * noise\n",
        "\n",
        "        # Predictor: Euler–Maruyama for Reverse SDE\n",
        "        noise = torch.randn_like(x)\n",
        "        g = sde.diffusion_coeff(t)\n",
        "        score = model(x, sigma)\n",
        "        x = x + (-g**2 * score) * dt + g * math.sqrt(-dt) * noise\n",
        "\n",
        "    return x.clamp(-1, 1)"
      ],
      "metadata": {
        "id": "2Y41cxMRJ1Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide Image Synthesis"
      ],
      "metadata": {
        "id": "M7haizm6NiZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sde = VESDE(sigma_min=0.01, sigma_max=50.0, N=1000)\n",
        "model = DDPMModel(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    ch=32,\n",
        "    ch_mult=(1, 2, 4),\n",
        "    num_res_blocks=2,\n",
        "    attn_resolutions={16},\n",
        "    dropout=0.1,\n",
        "    resamp_with_conv=True,\n",
        "    init_resolution=32\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "sample = sdedit_sampler(model, sde, x_guidance, 0.5, device=device, n_steps_pc=3)"
      ],
      "metadata": {
        "id": "zfwX1tV4J1Dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}