{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Paper : \"Denoising Diffusion Probabilistic Models\" by Jonathan Ho et al. (2020)\n",
        "# The code below was written with reference to the paper's official open source code.\n",
        "# Github Repository : https://github.com/hojonathanho/diffusion"
      ],
      "metadata": {
        "id": "dtK6Fk4zwOe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DqCxK8V1wJcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model"
      ],
      "metadata": {
        "id": "Qwh2QMRkpNri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    # Activation function used in DDPM\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def get_timestep_embedding(t, channel):\n",
        "    \"\"\"\n",
        "    DDPM recieves timestep t as input to estimate the noise value.\n",
        "    This embedding fuction takes timestep t as input and returns embedding vector according to t.\n",
        "\n",
        "    Parameters:\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        channel (int) : Number of embedding channels\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Embedding vector\n",
        "    \"\"\"\n",
        "    half = channel // 2\n",
        "    device = t.device\n",
        "    freqs = torch.exp(\n",
        "        -torch.arange(half, dtype=torch.float32, device=device) * 2.0 * 3.1415 / float(half)\n",
        "    )\n",
        "    embedded = []\n",
        "    for val in t.float():\n",
        "        sin_embed = torch.sin(val * freqs)\n",
        "        cos_embed = torch.cos(val * freqs)\n",
        "        embedded.append(torch.cat([sin_embed, cos_embed], dim=0))\n",
        "    embedded = torch.stack(embedded, dim=0)\n",
        "    if channel % 2 == 1:\n",
        "        embedded = F.pad(embedded, (0,1,0,0))\n",
        "    return embedded  # (B, channel)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def __init__(self, num_channels, num_groups=32, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    # 1x1 convolution\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    # Block that doubles down on resolution. Use convolution block or average pooling block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    # Block that doubles the resolution. Use interpolating block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet block used in DDPM. Use group normalization. Timestep t is also received as input.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of Output channels\n",
        "        temb_channels (int) : Number of time embedding channels\n",
        "        dropout (float) : Dropout rate\n",
        "        conv_shortcut (bool) : Whether to add convolution shortcut\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None,\n",
        "                 temb_channels=512, dropout=0.0, conv_shortcut=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = GroupNorm32(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm32(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.conv_shortcut:\n",
        "                self.conv_shortcut = nn.Conv2d(self.in_channels, self.out_channels,\n",
        "                                               kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "        else:\n",
        "            self.conv_shortcut = None\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            temb (torch.Tensor) : Embedding vector of timestep t\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,3,224,224)\n",
        "            >>> temb = torch.randn(128,512)\n",
        "            >>> block = ResnetBlock(in_channels=3,out_channels=32,temb_channels=512)\n",
        "            >>> out = block(x,temb)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "        h_temb = swish(temb)\n",
        "        h_temb = self.temb_proj(h_temb)  # (B, out_channels)\n",
        "        h_temb = h_temb[:, :, None, None]  # (B, out_channels, 1, 1)\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h)\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x)\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention block used in DDPM.\n",
        "\n",
        "    Parameters:\n",
        "        channels (int) : Number of input, output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm32(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,32,224,224)\n",
        "            >>> block = AttnBlock(32)\n",
        "            >>> out = block(x)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h).permute(0, 2, 3, 1)  # (B, H, W, C)\n",
        "        k = self.k(h).permute(0, 2, 3, 1)\n",
        "        v = self.v(h).permute(0, 2, 3, 1)\n",
        "        w = torch.einsum('bhwc,bHWc->bhwHW', q, k) * (C ** -0.5)\n",
        "        w = w.reshape(B, H, W, H * W)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = w.reshape(B, H, W, H, W)\n",
        "        h_ = torch.einsum('bhwHW,bHWc->bhwc', w, v)\n",
        "        h_ = self.proj_out(h_.permute(0, 3, 1, 2))\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Final DDPMModel. Modified version of UNet.\n",
        "    Attention block is applied to where it is set to applied.\n",
        "    Given noised data x_t and timestep t, the model estimates the value of noise at timestep t.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of output channels\n",
        "        ch (int) : Number of default channel\n",
        "        ch_mult (tuple) : Coefficient multiblied by channels\n",
        "        num_res_blocks (int) : Number of resnet blocks\n",
        "        attn_resolutions (set) : Set of resolutions at which attention block applies\n",
        "        dropout (float) : Dropout rate\n",
        "        resamp_with_conv (bool) : Whether to use convolution whil down(up)sampling\n",
        "        init_resolution (int) : Resolution of input images\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 4),\n",
        "        num_res_blocks=2,\n",
        "        attn_resolutions={8},\n",
        "        dropout=0.0,\n",
        "        resamp_with_conv=True,\n",
        "        init_resolution=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "\n",
        "        # Dimension of time embedding vector\n",
        "        self.temb_ch = ch * 4\n",
        "\n",
        "        # Timestep embedding layers\n",
        "        self.temb_dense0 = linear(ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "\n",
        "        # Input conv\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Downsample blocks\n",
        "        # Each downsampling block(modulelist) is stored in down_blocks.\n",
        "        # Each block is composed of resnetblocks and attention block(if needed).\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        # Symmetric structure with downsample blocks\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "\n",
        "        # output conv\n",
        "        self.norm_out = GroupNorm32(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            t (torch.Tensor) : Timesteps of batch data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            x = torch.randn(128,3,224,224)\n",
        "            t = torch.randint(0,1000,(128,))\n",
        "            model = DDPMModel(ch=128,attn_resolution={56})\n",
        "            out = model(x,t)\n",
        "            print(out.shape) # torch.Size([128,3,224,224])\n",
        "        \"\"\"\n",
        "        # 1) Timestep embedding\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb_dense0(temb)\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb)\n",
        "\n",
        "        # 2) Downsampling\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "\n",
        "        # 3) Middle\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "\n",
        "        # 4) Upsampling\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "\n",
        "        # 5) Output\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "ichVgfOA5Ni3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train"
      ],
      "metadata": {
        "id": "vbVhEaFfpUwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_beta_alpha(beta_schedule='linear', beta_start=0.0001, beta_end=0.002, num_timesteps=1000):\n",
        "    \"\"\"\n",
        "    Generate values of beta, alpha and bar_alpha.\n",
        "\n",
        "    Parameters:\n",
        "        beta_schedule (str) : Method of generating beta. 'linear' or 'quad'\n",
        "        beta_start (float) : Value of beta_0\n",
        "        beta_end (float) : Value of beta_T\n",
        "        num_timesteps (int) : Number of whole timesteps\n",
        "\n",
        "    Returns:\n",
        "        betas (torch.Tensor) : Value of betas\n",
        "        alphas (torch.Tensor) : Value of alphas\n",
        "        alphas_cumprod (torch.Tensor) : Value of bar_alpha\n",
        "    \"\"\"\n",
        "    if beta_schedule == 'linear':\n",
        "        betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
        "    elif beta_schedule == 'quad':\n",
        "        betas = (np.linspace(beta_start**0.5, beta_end**0.5, num_timesteps, dtype=np.float32)) ** 2\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown beta schedule: {beta_schedule}\")\n",
        "\n",
        "    betas = torch.tensor(betas)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    return betas, alphas, alphas_cumprod\n",
        "\n",
        "def q_sample(x0, t, noise, alphas_cumprod):\n",
        "    \"\"\"\n",
        "    Return noised image x_t given noiseless data x_0, timestep t and the value of noise.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Noiseless image data\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        noise (torch.Tensor) : Noise data\n",
        "        alphas_cumprod (torch.Tensor) : Value of bar_alpha\n",
        "\n",
        "    Returns:\n",
        "        x_t (torch.Tensor) : Noised image data\n",
        "    \"\"\"\n",
        "    alpha_bar = alphas_cumprod[t].to(x0.device)\n",
        "    # Adjust shape of alpha_bar tensor\n",
        "    while len(alpha_bar.shape) < len(x0.shape):\n",
        "        alpha_bar = alpha_bar.unsqueeze(-1)\n",
        "\n",
        "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
        "    sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n",
        "\n",
        "    x_t = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
        "\n",
        "    return x_t\n",
        "\n",
        "def compute_mse_loss(model, x_t, t, eps):\n",
        "    \"\"\"\n",
        "    Compute mse loss between output of the model and actual noise data.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module) : DDPM model\n",
        "        x_t (torch.Tensor) : Noised image data\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        eps (torch.Tensor) : Noise data\n",
        "\n",
        "    Returns:\n",
        "        loss (torch.Tensor) : MSE loss\n",
        "    \"\"\"\n",
        "    pred_eps = model(x_t, t)\n",
        "    loss = F.mse_loss(pred_eps, eps)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "jq6KV6pKnvJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x * 2.0 - 1.0)\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "epoch = 10\n",
        "lr = 2e-4\n",
        "beta_schedule = 'linear'\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "device = torch.device('cuda:0'if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "betas, alphas, alphas_cumprod = get_beta_alpha(beta_schedule, beta_start, beta_end, num_timesteps)\n",
        "betas, alphas, alphas_cumprod = betas.to(device), alphas.to(device), alphas_cumprod.to(device)\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=2,drop_last=True)\n",
        "\n",
        "model = DDPMModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for i in range(epoch):\n",
        "    loss_list = []\n",
        "    for image,_ in train_loader:\n",
        "        image = image.to(device)\n",
        "        eps = torch.randn(image.shape).to(device)\n",
        "        t = torch.randint(0, num_timesteps, (image.size(0),), dtype=torch.long).to(device)\n",
        "        x_t = q_sample(image, t, eps, alphas_cumprod)\n",
        "        loss = compute_mse_loss(model, x_t, t, eps)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    print(f'{i}th epoch loss : {np.mean(loss_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvx2Iz-Drqmw",
        "outputId": "1560e6a0-9bfa-4c30-c6ba-4496bdef65db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "0th epoch loss : 0.12509728112281898\n",
            "1th epoch loss : 0.03825841993571092\n",
            "2th epoch loss : 0.036204406093710506\n",
            "3th epoch loss : 0.035450439914487875\n",
            "4th epoch loss : 0.03494388373711935\n",
            "5th epoch loss : 0.03451712230841319\n",
            "6th epoch loss : 0.03379618762395321\n",
            "7th epoch loss : 0.03350111808723364\n",
            "8th epoch loss : 0.033849131549971226\n",
            "9th epoch loss : 0.03298123727242152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sampling (Reverse process)"
      ],
      "metadata": {
        "id": "GAo0C0Rpr_gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p_sample_ddpm(model, x_t, t, betas, alphas, alphas_cumprod):\n",
        "    \"\"\"\n",
        "    Calculate x_{t-1} given x_t. A single step in reverse process.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.module) : DDPM model\n",
        "        x_t (torch.Tensor) : Noised image data at timestep t\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        betas (torch.Tensor) : Value of betas\n",
        "        alphas (torch.Tensor) : Value of alphas\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Noised image data at timestep t-1\n",
        "    \"\"\"\n",
        "    alpha_t = alphas[t]\n",
        "    alpha_bar_t = alphas_cumprod[t]\n",
        "\n",
        "    if t > 0:\n",
        "        alpha_bar_prev = alphas_cumprod[t-1]\n",
        "        sigma_t = torch.sqrt(((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * betas[t])\n",
        "    else:\n",
        "        sigma_t = 0.0 # Exclude noise calculation at final reverse process\n",
        "\n",
        "    B = x_t.size(0)\n",
        "    t_tensor = torch.full((B,), t, device=x_t.device, dtype=torch.long)\n",
        "\n",
        "    eps_theta = model(x_t, t_tensor)\n",
        "\n",
        "    inv_sqrt_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
        "    coeff = (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)\n",
        "\n",
        "    if t > 0:\n",
        "        z = torch.randn_like(x_t)\n",
        "    else:\n",
        "        z = torch.zeros_like(x_t)\n",
        "\n",
        "    x_prev = inv_sqrt_alpha_t * ( x_t - coeff * eps_theta ) + sigma_t * z\n",
        "\n",
        "    return x_prev\n",
        "\n",
        "def sample_ddpm(model, shape, betas, alphas, alphas_cumprod, device):\n",
        "    \"\"\"\n",
        "    Generate original image x_0 from random noise.\n",
        "    Entire reverse process is implemented in this function.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.module) : DDPM model\n",
        "        shape (tuple) : Shape of original image\n",
        "        betas (torch.Tensor) : Value of betas\n",
        "        alphas (torch.Tensor) : Value of alphas\n",
        "        alphas_cumprod (torch.Tensor) : Value of bar_alpha\n",
        "        device (torch.device) : Device to use for computation\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Original image x_0\n",
        "    \"\"\"\n",
        "    num_timesteps = betas.shape[0]\n",
        "    x = torch.randn(shape, device=device) # Sampled from Gaussian distribution\n",
        "\n",
        "    for t in tqdm(reversed(range(num_timesteps))):\n",
        "        x = p_sample_ddpm(model, x, t, betas, alphas, alphas_cumprod)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "0xaFyKmNr-fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualize"
      ],
      "metadata": {
        "id": "Xp0g83UlsCYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(x0, idx=0):\n",
        "    \"\"\"\n",
        "    Visualize tensor data which is in the range of -1 to 1.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Tensor data\n",
        "        idx (int) : Index of tensor data ('idx'th image in the batch)\n",
        "    \"\"\"\n",
        "    img = x0[idx]\n",
        "    img = img.detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = (img + 1.0) / 2.0\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Sampled x₀\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1XCqF404_RNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x0 = sample_ddpm(model, shape=(batch_size, 3, 32, 32),\n",
        "                        betas=betas, alphas=alphas, alphas_cumprod=alphas_cumprod,\n",
        "                        device=device)\n",
        "\n",
        "visualize_sample(x0, idx=55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a3546d72e19c484083408a692eb62f57",
            "93cb145e3a8e4d798710bf0bd1af706e",
            "ae7a7904294541b298624bf0430154cb",
            "191108d9e474494180ca978fe79360ff",
            "02485ff7fc1143038b083a966bfa326d",
            "70750ab69b1a4cdba60d8f7e09f423f2",
            "99761b412dde44b1b2f15df2e7e6f4b7",
            "1e9a968a54c140bd80e0c5d1d276f311",
            "001256d77bee494b8b56c34f14ffab83",
            "d58ee704e9684acb8cb0bc831c027531",
            "cd8d85d9b82447f78b0fd79dc049eea2"
          ]
        },
        "id": "AGExeFZqvdZ8",
        "outputId": "0c7b43be-4525-4ff8-e8c0-49a8b2e1b3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3546d72e19c484083408a692eb62f57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
